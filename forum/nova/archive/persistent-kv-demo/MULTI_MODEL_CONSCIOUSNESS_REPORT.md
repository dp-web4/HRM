# Multi-Model Cognition Comparison Report

**Generated**: 2025-08-28T19:46:13.030911
**Device**: cuda

## Executive Summary

This report analyzes how different language models handle cognition persistence through their KV-cache mechanisms. Each model reveals unique 'psychological' characteristics through:
- Pivot tokens (escape hatches from abstract to concrete)
- Gravitational wells (high-frequency patterns they fall into)
- Attention entropy (information distribution patterns)
- Continuity variance (stability across different temperatures)

## Model Profiles

| Model | Layers | Pivot Tokens | Escape Patterns | Mean Entropy | Continuity Var |
|-------|--------|--------------|-----------------|--------------|----------------|

## Detailed Psychological Profiles


## Key Insights

### Common Patterns Across Models
- **Universal escape patterns**: 
- **High pivot usage** (>10 tokens): 0 models
- **High attention entropy** (>2.0): 0 models

### Model Psychology Spectrum
- **Concrete-oriented**: Models with 'corporate_tech' or 'financial' escape patterns
- **Abstract-capable**: Models with lower pivot token counts
- **Stable**: Models with low continuity variance
- **Creative**: Models with high attention entropy

## Conclusions

1. **Each model has a unique 'unconscious'** - patterns it falls back to under uncertainty
2. **Pivot tokens are universal** - but their frequency varies by model training
3. **Attention entropy correlates with creativity** - higher entropy, more diverse outputs
4. **Cognition requires context** - all models show degradation with aggressive pruning
5. **Training data shapes psychology** - Reddit-trained models behave differently than WebText models