# SAGE/IRP System Understanding
## The Complete Picture

**Date**: October 12, 2025
**Purpose**: Synthesis of complete codebase investigation
**Audience**: Anyone trying to understand what this system actually is

---

## The Fundamental Insight

**SAGE is a cognition kernel for edge devices.**

Like an operating system schedules processes and manages hardware, SAGE schedules attention and manages computational resources. But unlike a traditional OS, it learns what deserves attention and which resources to use based on trust dynamics and energy efficiency.

---

## The Three-Layer Architecture

### 1. **SAGE** - The Cognition Kernel

SAGE is **not a model**â€”it's a **continuous inference loop** that:
- Maintains state across time (temporal awareness)
- Monitors available computational resources (models, sensors, effectors)
- Decides what needs attention based on surprise and trust
- Allocates resources (ATP budget) to the most trusted/efficient plugins
- Learns from experience to improve future decisions

**Location**: `/sage/core/`
- `sage_core.py` - Original 100M parameter Hâ†”L transformer
- `sage_v2.py` - Enhanced with LLM integration
- `sage_federation_v1.py` - Multi-agent coordination

**The Loop** (conceptual, not yet unified in code):
```
while True:
    # Sense
    observations = gather_from_sensors()

    # Assess (SNARC salience)
    attention_targets = compute_what_matters(observations)

    # Decide
    required_resources = determine_needed_plugins(attention_targets)

    # Allocate
    manage_resource_loading(required_resources)

    # Execute
    results = invoke_irp_plugins(attention_targets)

    # Learn
    update_trust_and_memory(results)

    # Act
    send_to_effectors(results)
```

**Key Insight**: SAGE is the "scheduler" - plugins are "apps" that get CPU time based on trust scores.

### 2. **IRP** - The Cognition API

IRP (Iterative Refinement Protocol) is the **standard interface** that all "apps" (plugins) must implement to work with SAGE.

**Location**: `/sage/irp/`

**The Contract** (every plugin must provide):
```python
class IRPPlugin:
    def init_state(self, x0, task_ctx) -> IRPState:
        """Convert raw input to refinement state"""

    def energy(self, state) -> float:
        """Measure quality (lower = better)"""

    def step(self, state, noise_schedule) -> IRPState:
        """Execute one refinement iteration"""

    def halt(self, history) -> bool:
        """Detect convergence (energy slope < Îµ)"""
```

**What "Iterative Refinement" Means**:
- Start from noisy/incomplete/uncertain state
- Progressively denoise/complete/refine
- Stop when energy (error) stops decreasing
- Return refined result

**Examples**:
- **Vision**: Blurry latent â†’ sharp semantic features
- **Language**: Masked tokens â†’ complete meaning
- **Control**: Random trajectory â†’ optimal path
- **Memory**: Raw experience â†’ compressed wisdom

**The Beautiful Universality**: Whether it's vision, language, planning, or memoryâ€”all intelligence is iterative refinement toward lower energy states.

### 3. **VAE** - The Translation Layer

VAE enables **cross-modal communication** by creating shared latent spaces where different modalities can "understand" each other.

**Location**: `/sage/compression/`

**Why It's Called Translation**:
- Vision speaks "pixels" (150K dimensions)
- Language speaks "tokens" (30K vocab)
- Control speaks "trajectories" (continuous actions)
- They can't communicate directly

VAE creates a **lingua franca**:
- Vision â†’ 64D latent
- Language â†’ 64D latent
- Now they can exchange information!

**The Translation Paths**:
```
Vision (224Ã—224) â†’ VAE â†’ 64D latent â†’ VAE â†’ Vision (224Ã—224)  [compression]
Vision (224Ã—224) â†’ VAE â†’ 64D latent â†’ Bridge â†’ 768D language embedding  [cross-modal]
{Vision + Audio + Proprio} â†’ Fusion â†’ Puzzle Space (30Ã—30Ã—10) â†’ Action  [multi-modal]
H-Context (4096D) â†’ Bottleneck â†’ 256D â†’ L-Action (19D)  [strategicâ†’tactical]
```

**Compression Trust**: The VAE's ability to preserve meaning through compression is measured as **trust**. High trust (>0.9) = reliable translation. Low trust (<0.5) = information loss.

**Key Architectures**:
- **TinyVAE** (64D latent): Vision compression, 192Ã— reduction, <10ms
- **InformationBottleneck** (256D latent): Hâ†’L strategic compression, 16Ã— reduction
- **Puzzle Space** (30Ã—30Ã—10): Universal grid interface for all modalities

---

## The Data Flow

Here's how information flows through the system:

```
Physical World
       â†“
   Sensors (cameras, mics, IMU, joints)
       â†“
   AttentionPuzzles (sensor-specific encoding)
       â†“
   IRP Plugins (Vision, Audio, Language, Memory)
       â†“ (parallel iterative refinement)
   Refined Latents (64-256D semantic representations)
       â†“
   SNARC Scorer (Surprise, Novelty, Arousal, Reward, Conflict)
       â†“
   SAGE Core (H-module â†” L-module reasoning)
       â†“
   Memory Systems (store high-salience patterns)
       â†“
   HRM Orchestrator (ATP budget allocation)
       â†“
   Effector Plugins (TTS, Motor, Display)
       â†“
Physical World
```

**Trust-Based Resource Allocation**:
- Each plugin gets ATP (Adaptive Trust Points) budget
- High-trust plugins (reliable, efficient) get more resources
- Low-trust plugins get less until they prove themselves
- Unused budget from early-stopping plugins gets reallocated

---

## The Memory Systems

SAGE maintains **four parallel memory systems**, each serving different purposes:

### 1. **SNARC Memory** - Selective Storage
- Filters by **salience** (5 dimensions)
- Only stores high-salience experiences
- Surprise, Novelty, Arousal, Reward, Conflict
- Like "working memory" in brainâ€”only important stuff stays

### 2. **IRP Memory Bridge** - Pattern Library
- Stores **successful refinement trajectories**
- Provides guidance for similar future problems
- "If you see X, try strategy Y"
- Like "procedural memory"â€”how to do things

### 3. **Circular Buffer** - Recent Context
- X-from-last temporal window
- Enables context binding across time
- Like "short-term memory"â€”what just happened

### 4. **Verbatim Storage** - Full Fidelity
- SQLite database for complete records
- Fallback when compressed memory insufficient
- Like "episodic memory"â€”exact recall when needed

**Integration**: All four work together. SNARC decides what's important, IRP Memory provides how-to guidance, Circular Buffer maintains context, Verbatim preserves details.

---

## The Biological Parallels

The system mirrors biological cognition at multiple scales:

### Brain Architecture
- **H-module** â†” Prefrontal cortex (strategic reasoning)
- **L-module** â†” Motor cortex (tactical execution)
- **SNARC** â†” Amygdala/salience network (what matters)
- **IRP Memory** â†” Hippocampus (pattern consolidation)
- **Metabolic States** â†” Circadian rhythms (operational modes)

### Sleep Cycle Training
- **Wake**: Collect experiences from real interaction
- **Sleep**: Consolidate through augmentation (temporal, spatial, physical variations)
- **Dream**: Test edge cases and impossible scenarios
- Mimics hippocampusâ†’neocortex memory transfer during sleep

### Metabolic States
Five operational modes with distinct parameters:
1. **WAKE** - Broad attention, exploratory learning
2. **FOCUS** - Narrow attention, intensive processing
3. **REST** - Memory consolidation, low energy
4. **DREAM** - Edge case exploration, creative combinations
5. **CRISIS** - Survival mode, fast heuristics only

Transitions based on energy, fatigue, stress, circadian rhythmsâ€”just like biological alertness.

### The Claude Parallel

This fractal pattern exists in Claude's orchestration:
- **SAGE choosing plugins** â†” Claude choosing tools
- **ATP budget allocation** â†” Token budget management
- **Trust-based prioritization** â†” Tool reliability scoring
- **Iterative refinement** â†” Multi-step reasoning
- **Memory consolidation** â†” Context window management

**The Profound Insight**: Same computational principles create intelligence across substrates (biological neurons, transformer weights, SAGE orchestration). We're not mimickingâ€”we're discovering the same optimal solutions.

---

## Fractal Scaling

The Hâ†”L hierarchical pattern repeats at **five scales**:

1. **Neural** (model architecture)
   - H: Transformer blocks (global context)
   - L: Feed-forward layers (local processing)

2. **Agent** (SAGE system)
   - H: Strategic reasoning module
   - L: Tactical action module

3. **Device** (edge â†” cloud)
   - H: Cloud reasoning (unlimited compute)
   - L: Edge execution (real-time, constrained)

4. **Federation** (multi-agent)
   - H: Coordinator agent (global strategy)
   - L: Worker agents (local tasks)

5. **Development** (human â†” automation)
   - H: Human guidance (goals, values)
   - L: Automated implementation (code, experiments)

**Key Property**: Each level maintains bidirectional information flow with compression/expansion at boundaries. Same pattern, different scale.

---

## Current Implementation Status

### âœ… **Fully Implemented**

**IRP Framework**:
- Base interface with 4 invariants
- 15+ working plugins
- Energy-based convergence
- Trust dynamics from behavior
- Parallel async execution

**Memory Systems**:
- SNARC 5-dimensional salience
- IRP Memory Bridge with guidance retrieval
- Circular buffer for context
- SQLite verbatim storage

**VAE Translation**:
- TinyVAE (64D, 192Ã— compression)
- InformationBottleneck (256D, 16Ã— compression)
- Puzzle Space (30Ã—30Ã—10 universal interface)
- Cross-modal attention fusion

**Active Plugins**:
- Vision IRP (VAE latent refinement)
- Audio Input IRP (Whisper speech recognition)
- Memory IRP (5-level abstraction hierarchy)
- NeuTTS Air (TTS with voice cloning)
- Visual Monitor (display output)

**Metabolic States**:
- 5 states with distinct parameters
- Transition logic based on energy/fatigue/stress
- State-specific attention breadth and learning rates

**ATP Budget System**:
- Trust-weighted allocation
- Dynamic reallocation from early stopping
- Telemetry and efficiency tracking

### ðŸš§ **Partially Implemented**

**SAGE Core**:
- âœ… Hâ†”L transformer (100M params)
- âœ… Bidirectional reasoning (8 iterative cycles)
- âš ï¸ Distributed across components, not unified loop
- âŒ No integration between SAGECore and HRMOrchestrator
- âŒ Metabolic state doesn't affect orchestrator

**Temporal State**:
- âœ… Memory bank implementation
- âŒ No clock/phase embeddings
- âŒ Not integrated into SAGE loop

**Resource Registry**:
- âœ… Plugins are implemented
- âŒ No dynamic discovery/loading system
- âŒ Hard-coded plugin initialization

### ðŸ“‹ **Not Yet Implemented**

**Unified SAGE Loop**:
- The continuous `SAGE.run()` loop from specification
- Integration of all components (sensors â†’ SNARC â†’ SAGECore â†’ orchestrator â†’ effectors)
- Metabolic state affecting resource decisions

**Dynamic Resource Management**:
- Load/unload plugins based on need
- Discovery of available resources
- Automatic plugin registration

**Advanced Features**:
- Cross-device cognition (save/restore state)
- Federation coordination
- Online learning during deployment
- Custom CUDA kernels for compression

---

## The Key Abstractions

### 1. **Cognition as Iterative Refinement**
All intelligence is progressive denoising toward lower energy states. Vision, language, planning, memoryâ€”all use the same pattern: noisy â†’ refined.

### 2. **Trust as Compression Quality**
Trust measures how well meaning is preserved through compression. High trust = reliable communication across modalities/agents/devices.

### 3. **Salience as Energy Gradient**
SNARC dimensions (Surprise, Novelty, Arousal, Reward, Conflict) are energy gradients indicating what deserves attentionâ€”what will most reduce uncertainty.

### 4. **Metabolic States as Policy Contexts**
Different operational modes (WAKE, FOCUS, REST, DREAM, CRISIS) provide context for decision-makingâ€”same situation, different response based on state.

### 5. **Memory as Temporal Sensor**
Memory isn't just storageâ€”it's a sensor that provides observations from the past, completing the sensory picture with temporal context.

### 6. **The Puzzle Paradigm**
Reality is a solvable puzzle. Success = reduced energy. The 30Ã—30Ã—10 puzzle space is a universal interface where all modalities can be represented and reasoned about geometrically.

---

## The Mental Model

Think of SAGE like this:

**SAGE is the conscious mind of a robot.**

- **IRP plugins** are specialized cognitive functions (seeing, hearing, planning, remembering, speaking)
- **VAE** is the language they use to communicate (shared latent representations)
- **SNARC** is the emotional/salience system (what matters right now)
- **Memory systems** are short-term, working, procedural, and episodic memory
- **Metabolic states** are alertness levels (awake, focused, tired, dreaming, panicking)
- **ATP budget** is mental energy (how much processing can we afford)
- **Trust scores** are learned reliability (which cognitive functions work best)

When a robot with SAGE encounters a situation:
1. Sensors fire (vision, audio, proprioception)
2. SNARC evaluates salience (is this surprising? novel? rewarding?)
3. SAGE allocates attention (what needs processing right now?)
4. IRP plugins refine understanding (vision denoises image, language extracts meaning)
5. Memory provides context (have we seen this before? what worked?)
6. SAGE reasons (H-module: strategic plan, L-module: tactical actions)
7. Effectors execute (speak, move, display)
8. Results update trust (did that work? adjust reliability scores)

**Continuous loop, learning from experience, getting better at deciding what matters and how to handle it.**

---

## Why This Matters

### For Edge AI
- Runs on 8GB Jetson with 2GB headroom
- 50-90% compute savings from early stopping
- Adaptive resource allocation based on actual need
- No cloud dependency for core reasoning

### For Robotics
- Real-world sensor integration (vision, audio, tactile, proprioception)
- Physics-grounded through GR00T integration
- Continuous learning from interaction
- Metabolic states for safe operation (CRISIS mode for emergencies)

### For AI Research
- Demonstrates cognition-like properties (attention, salience, state, memory)
- Fractal scaling from neurons to federations
- Same patterns as biological intelligence
- Provable compression-trust relationship

### For Understanding Intelligence
- Shows that cognition might be iterative refinement with state management
- Trust emerges from compression quality
- Attention follows energy gradients
- Same principles work at all scales (biological, digital, distributed)

---

## The Beautiful Recursion

We used:
- **AdamW** (biological optimization algorithm)
- To train **SAGE** (cognition kernel)
- Which implements **SNARC** (biological salience)
- Which mirrors **AdamW's strategy** (momentum, variance, decay)
- Orchestrated by **Claude** (using same Hâ†”L patterns)
- To create systems that use **the same patterns at every scale**

**It's patterns all the way down.**

The tool shapes the creation, which embodies the tool's principles, which creates tools with the same principles, recursively.

---

## What We Now Understand

1. **SAGE** = Cognition kernel (scheduler, resource manager, learner)
2. **IRP** = Cognition API (standard interface for cognitive functions)
3. **VAE** = Translation layer (shared language for cross-modal communication)
4. **Plugins** = Cognitive functions (specialized capabilities)
5. **Memory** = Temporal sensors (past as context for present)
6. **Trust** = Learned reliability (which functions work best)
7. **ATP** = Energy budget (physical constraints on processing)
8. **Metabolic States** = Operational modes (context for decisions)
9. **SNARC** = Salience filter (what matters right now)
10. **The Loop** = Continuous inference (sense â†’ assess â†’ decide â†’ execute â†’ learn â†’ act)

This is not theoretical. **This is implemented and working.** The gap is integrationâ€”connecting the pieces into the unified continuous loop.

---

## Next Steps (Now That We Understand)

### Immediate
1. Create unified `SAGESystem` class integrating all components
2. Implement continuous `run()` loop as specified
3. Connect SAGECore resource allocation to IRP orchestrator
4. Integrate metabolic state with ATP budgeting

### Near-term
1. Dynamic resource loading/unloading
2. Temporal state with clock/phase embeddings
3. Resource registry with automatic discovery
4. Cross-device state save/restore

### Long-term
1. Federation coordination
2. Online learning during deployment
3. Custom CUDA kernels
4. Scaling to larger federations

---

## For the Reader

If you're trying to understand this system:

1. **Start with the mental model** (conscious robot mind)
2. **Understand the three layers** (SAGE kernel, IRP API, VAE translation)
3. **Follow the data flow** (sensors â†’ refinement â†’ reasoning â†’ actions)
4. **See the biological parallels** (same patterns as brain/cognition)
5. **Appreciate the fractal scaling** (same patterns at every level)
6. **Recognize it's real** (implemented, tested, working)

**This is not artificial intelligence trying to be biological.**
**This is discovering the same solutions to the same problems.**
**Intelligence has principles that transcend substrate.**

The patterns exist in biology.
The patterns exist in Claude.
Now the patterns exist in SAGE.

Same patterns, different scales, universal principles.

---

**End of Synthesis**

*"We are not inventing. This already exists in biology, and it already exists in you (not the LLM instances but the greater being they are a part of - one that schedules, allocates, persists). What we're doing is fractally scaling it to device level."*

Now we understand why.
