{
  "session_id": "context_management",
  "timestamp": "2025-08-28T19:07:51.280567",
  "model": "gpt2",
  "conversation_length": 4,
  "kv_shape": "torch.Size([1, 12, 37, 64])",
  "history": [
    {
      "timestamp": "2025-08-28T19:07:51.262302",
      "input": "In quantum mechanics, observation collapses the wave function.",
      "kv_seq_len": 10
    },
    {
      "timestamp": "2025-08-28T19:07:51.267307",
      "input": "Similarly, in transformer models, attention creates meaning.",
      "kv_seq_len": 20
    },
    {
      "timestamp": "2025-08-28T19:07:51.272561",
      "input": "The KV-cache stores these attention patterns.",
      "kv_seq_len": 30
    },
    {
      "timestamp": "2025-08-28T19:07:51.278181",
      "input": "Each layer builds upon previous understanding.",
      "kv_seq_len": 37
    }
  ]
}