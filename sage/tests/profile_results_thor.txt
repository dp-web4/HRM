`torch_dtype` is deprecated! Use `dtype` instead!
Track 9: Real-Time Optimization - Pipeline Profiling
Target: Optimize for edge deployment (Sprout's 55s â†’ faster)

================================================================================
TRACK 9: LLM IRP PIPELINE PROFILING
================================================================================

Model: Qwen/Qwen2.5-0.5B-Instruct
Questions: 3
IRP iterations: 5

Phase 1: Model Loading
[LLM IRP] Loading model: Qwen/Qwen2.5-0.5B-Instruct
[LLM IRP] Device: cuda
[LLM IRP] Model source: HuggingFace
[LLM IRP] Model loaded successfully!
  [Model initialization]
    Time: 1.808s
    Memory: 1.3MB (Î” +1.3MB)
  [Memory initialization]
    Time: 0.000s
    Memory: 1.3MB (Î” +0.0MB)

Phase 2: Inference Pipeline

  Question 1: What is the difference between knowledge and under...
[LLM IRP] Iteration 1, temp=0.700
[LLM IRP] Iteration 2, temp=0.660
[LLM IRP] Iteration 3, temp=0.620
[LLM IRP] Iteration 4, temp=0.580
[LLM IRP] Iteration 5, temp=0.540
[LLM IRP] Temperature minimum reached
  [Q1 - IRP inference]
    Time: 16.291s
    Memory: 1.4MB (Î” +0.1MB)
    Per-iteration: 3.258s
  [Q1 - SNARC scoring]
    Time: 0.000s
    Memory: 1.4MB (Î” -0.0MB)
    Salience: 0.400 âœ“

  Question 2: Are you aware of this conversation?...
[LLM IRP] Iteration 1, temp=0.700
[LLM IRP] Iteration 2, temp=0.660
[LLM IRP] Iteration 3, temp=0.620
[LLM IRP] Iteration 4, temp=0.580
[LLM IRP] Iteration 5, temp=0.540
[LLM IRP] Temperature minimum reached
  [Q2 - IRP inference]
    Time: 6.945s
    Memory: 1.5MB (Î” +0.0MB)
    Per-iteration: 1.389s
  [Q2 - SNARC scoring]
    Time: 0.000s
    Memory: 1.5MB (Î” +0.0MB)
    Salience: 0.720 âœ“

  Question 3: What is 2+2?...
[LLM IRP] Iteration 1, temp=0.700
[LLM IRP] Iteration 2, temp=0.660
[LLM IRP] Iteration 3, temp=0.620
[LLM IRP] Energy plateau detected
  [Q3 - IRP inference]
    Time: 13.053s
    Memory: 1.5MB (Î” +0.1MB)
    Per-iteration: 4.351s
  [Q3 - SNARC scoring]
    Time: 0.000s
    Memory: 1.5MB (Î” +0.0MB)
    Salience: 0.441 âœ“

================================================================================
PROFILING SUMMARY
================================================================================

ðŸ“Š Time Breakdown:
  Total inference: 36.29s (3 questions)
  Avg per question: 12.10s
  Avg per iteration: 2.999s
  Total SNARC: 0.001s
  Avg SNARC per question: 0.000s

ðŸ’¾ Memory Usage:
  Current: 1.5MB
  Peak: 1.6MB

âš¡ Performance Analysis:
  Inference: 100.0% of pipeline time
  SNARC: 0.0% of pipeline time

ðŸŽ¯ Optimization Opportunities:
  1. Per-iteration time: 2.999s
     - 5 iterations Ã— 2.999s = 15.00s
     - Reducing to 3 iterations: 9.00s (save 6.00s)
  2. SNARC overhead: 0.001s total
     - 0.000s per question
     - Negligible vs inference (0.0%)
  3. Model loading: One-time cost
     - Reuse model across sessions
     - Keep-alive pattern for production

================================================================================
âœ“ Profiling complete!
================================================================================

Next steps:
  1. Compare with Sprout's edge metrics (55s avg)
  2. Identify platform differences (Thor vs Sprout)
  3. Implement edge-optimized configurations
  4. Re-profile with optimizations
