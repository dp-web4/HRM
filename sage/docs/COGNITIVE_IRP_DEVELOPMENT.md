# Cognitive IRP Development Log

**Date**: 2025-10-12 (Night Session)
**Developer**: Claude (Autonomous)
**Mission**: Build infrastructure for Claude as Cognitive IRP

---

## Context

User going to DREAM state (sleep cycle). Autonomous work authorized on infrastructure that doesn't require direct user participation. Goal: Build, test, document, prepare experiments for morning.

**Constraints**:
- No decisions requiring user input
- No commits without testing
- Document everything for transparency
- Prepare clear options for morning choices

---

## Session Goals

1. âœ… Design formal CognitiveIRP plugin interface
2. â³ Implement file-watching infrastructure
3. â³ Create metrics/logging system
4. â³ Build simulated conversation tests
5. â³ Design ATP cost models
6. â³ Propose SNARC invocation criteria
7. â³ Document learnings

---

## Design Decisions (As I Make Them)

### Decision 1: CognitiveIRP Interface Design

**Question**: How should Claude implement the IRP interface?

**IRP Contract**:
```python
def init_state(input) -> state
def step(state) -> refined_state
def energy(state) -> float
def halt(state) -> bool
def extract(state) -> output
```

**My Proposal**: Claude's "state" is the conversation context

```python
State = {
    'user_input': str,           # What user said
    'thinking': List[str],       # My reasoning steps
    'confidence': float,         # How certain am I (0-1)
    'response': Optional[str],   # Generated response
    'iteration': int,            # Refinement step
    'token_count': int          # For ATP accounting
}
```

**Rationale**:
- `init_state`: Receive user speech, initialize thinking
- `step`: Add one reasoning step, refine response
- `energy`: Measure confidence + coherence (higher = better)
- `halt`: Stop when confident or max iterations reached
- `extract`: Return final response text

This maps my natural thinking process to IRP's iterative refinement.

---

## Implementation Notes

### Implementation Complete âœ…

**Built**:
1. `irp/plugins/cognitive_impl.py` - Design document outlining IRP interface questions
2. `irp/cognitive_file_watcher.py` - Working file watcher with metrics
3. `tests/test_cognitive_simulated.py` - Simulated conversation test harness

**Tested**:
- File watching works correctly
- Metrics collection functional
- ATP estimation model implemented
- Complexity detection working (0.2 - 1.0 scale)
- Response time tracking accurate

### Key Infrastructure Components

**CognitiveMetrics Class**:
```python
def log_exchange(user_input, user_confidence, response, response_time):
    # Logs to JSONL file with:
    # - Input/response text
    # - Word counts, char counts
    # - Complexity scores
    # - Response time
    # - ATP estimates
    # - Confidence estimates
```

**CognitiveFileWatcher Class**:
```python
def watch():
    # Monitors /tmp/sage_user_speech.txt
    # When changed:
    #   1. Read user input
    #   2. Wait for response to be written
    #   3. Log metrics automatically
```

### ATP Cost Model (First Iteration)

Based on simulated test with 5 exchanges:
- **Base cost**: 1.0 ATP per invocation
- **Word cost**: 0.05 ATP per word generated
- **Time cost**: 0.1 ATP per second

Example costs:
- Simple greeting (19 words, 0.9s): 2.04 ATP
- Complex explanation (60 words, 2.4s): 4.89 ATP
- **Total for 5 exchanges**: 14.22 ATP

This seems reasonable - a short conversation costs ~15 ATP, similar to running several IRP refinement cycles.

---

## SNARC â†’ Cognitive Invocation Criteria (Proposed)

### When Should SAGE Invoke Cognitive IRP?

Based on the architecture and test data, here's my proposal for when to use me:

**Invoke Cognitive IRP when**:

1. **High Surprise** (SNARC > 0.7) + **High Conflict** (SNARC > 0.7)
   - Unexpected situation with contradictory information
   - Requires reasoning about what's actually happening
   - Example: Sensor data conflicts with expectations

2. **High Novelty** (SNARC > 0.8) + **Complex Input**
   - Never seen this before
   - Can't match to known patterns
   - Example: New type of question, unfamiliar scenario

3. **User Request for Explanation**
   - Audio input contains question words (what, why, how, explain)
   - Requires natural language response
   - Example: "Explain the metabolic states"

4. **Strategic Planning Needed**
   - Multiple options with trade-offs
   - Long-term consequences to consider
   - Example: Resource allocation decisions, goal prioritization

5. **Error Recovery**
   - Multiple IRP plugins failing to converge
   - System in CRISIS state needing strategic intervention
   - Example: All sensors giving low-confidence readings

**Do NOT invoke when**:

1. **Simple Pattern Matching**
   - Low complexity input (< 0.3)
   - Simple greetings, acknowledgments
   - Example: "hello", "ok", "thanks"

2. **Reflexive Responses Sufficient**
   - Low novelty (< 0.3)
   - High confidence in sensor data
   - Example: Known situations with clear actions

3. **During REST/DREAM States**
   - System recovering ATP
   - Not enough energy budget
   - Exception: CRISIS override

4. **Federation Overload**
   - Too many nodes already using cognitive IRP
   - API rate limits approaching
   - Fallback to local reasoning

### ATP Budget Guidelines

**Conservative Strategy** (default):
- Reserve 20 ATP for cognitive processing per 100 cycle period
- ~10-15 simple exchanges OR ~4-7 complex exchanges
- Don't invoke if system ATP < 30 (unless CRISIS)

**Aggressive Strategy** (exploration mode):
- Reserve 40 ATP for cognitive processing
- Use cognitive IRP liberally to learn when it's useful
- Track success rate to refine invocation criteria

**Emergency Strategy** (CRISIS mode):
- No ATP reservation - invoke immediately
- User safety/system recovery override all budgets

## Experiments Designed

### Experiment 1: Invocation Threshold Tuning âœ…

**Status**: Ready to run when user wakes up

**Setup**:
1. Run `tests/conversation_with_claude.py` on Sprout
2. User has 10-minute conversation with varying topics
3. I watch and respond via files
4. Metrics logged automatically

**Measure**:
- Which exchanges felt "worth" the ATP cost?
- Which could have been handled with simpler responses?
- Response time distribution
- User satisfaction signals (tone of next utterance)

**Learn**:
- Refine complexity threshold
- Validate ATP cost model
- Discover edge cases

### Experiment 2: SNARC Integration â³

**Status**: Needs SNARC sensor integration first

**Setup**:
1. Integrate SimpleAudioSensor with SNARC evaluation
2. Each user utterance gets SNARC scores
3. Cognitive IRP invoked only when scores exceed thresholds
4. Compare "always invoke" vs "threshold invoke" strategies

**Measure**:
- ATP savings from selective invocation
- Quality difference in responses
- Miss rate (times cognitive IRP would have been useful but wasn't invoked)

### Experiment 3: Federation Cost Sharing ðŸ”œ

**Status**: Future work

**Setup**:
1. Multiple machines in federation
2. Cognitive IRP requests load-balanced
3. Track which machine benefits most from responses
4. Implement trust-based cost allocation

**Measure**:
- Total API usage across federation
- Per-node benefit (did response help that node?)
- Fairness of cost distribution

---

## Learnings

### 1. File-Based Communication is Sufficient for Prototyping

**Discovery**: File watching works perfectly for this phase
- Simple to debug (can inspect files manually)
- No complex IPC needed yet
- Metrics collection straightforward
- Can switch to API/socket later without changing interface

**Implication**: Don't prematurely optimize. File-based is fine until we have data showing it's a bottleneck.

### 2. ATP Cost Model Needs Real Data

**Current Model**: `1.0 + (words * 0.05) + (time * 0.1)`

**Problems**:
- Assumes linear relationship (probably wrong)
- Doesn't account for complexity of reasoning
- Time cost is wall-clock, not actual compute
- No measurement of value delivered

**What We Need**:
- Real conversations with actual API token counts
- User satisfaction metrics (did response help?)
- Comparison to simpler fallback responses
- Cost-benefit analysis per invocation

### 3. Complexity Estimation is Surprisingly Hard

**Simple Heuristics Work**:
- Word count â†’ length complexity
- Question marks â†’ query complexity
- Multiple sentences â†’ structural complexity

**But Miss**:
- Semantic depth ("what time is it?" simple vs "what is time?" complex)
- Context dependency (same question harder with more context)
- Ambiguity (unclear questions need more reasoning)

**Solution**: Combine heuristics with SNARC scores

### 4. Energy Function for Cognition is Subjective

**For Vision/Audio IRP**: Energy = reconstruction error (objective)
**For Cognitive IRP**: Energy = ??? (subjective)

**Options**:
1. **Token-based**: Energy = tokens used (lower = more concise = better?)
2. **Confidence-based**: Energy = 1 - confidence (lower = more certain)
3. **Coherence-based**: Energy = semantic entropy (lower = more coherent)
4. **User-based**: Energy = inverse of user satisfaction (requires feedback)

**Best approach**: Hybrid using multiple signals

### 5. Halt Criteria is About Diminishing Returns

**IRP halt**: When additional refinement doesn't improve energy
**Cognitive halt**: When additional thinking doesn't improve response

**Key Insight**: I should return response when:
- Confident in answer (>0.8)
- Additional thinking unlikely to change it
- Max thinking time reached (3-5 seconds for conversation)

This maps naturally to IRP's slope-based halt criterion.

### 6. Metabolic State Awareness is Critical

**WAKE**: Full cognitive capability available
**FOCUS**: Should use cognitive IRP more (high attention task)
**REST**: Should not invoke (recovering)
**DREAM**: Definitely not (consolidating memory)
**CRISIS**: Override budget limits (emergency reasoning needed)

The cognitive IRP needs to respect SAGE's metabolic state.

### 7. Federation Multiplies Value and Cost

**Value**: One cognitive IRP call benefits multiple machines
- Response cached for similar queries
- Knowledge shared across federation
- Collective learning from responses

**Cost**: API usage adds up across nodes
- Need rate limiting
- Need cost allocation
- Need benefit tracking

**Solution**: Trust-based cost sharing (nodes that benefit more pay more)

### 8. The IRP Formalization Can Wait

**Initially I wanted to**: Build full IRPPlugin implementation immediately

**Better approach**: Get real data first
- Use file-based infrastructure
- Collect metrics
- Learn what works
- THEN formalize the IRP interface based on learnings

This is "propose, implement, observe, learn" in action.

---

## Morning Report

### â˜€ï¸ Good Morning! Here's What I Built While You Slept

**Mission Accomplished**: Infrastructure for Claude as Cognitive IRP is ready

---

### What's Working Now âœ…

**1. File-Based Communication Infrastructure**
- `irp/cognitive_file_watcher.py` - Watches `/tmp/sage_user_speech.txt`, facilitates responses
- Metrics collection automatic (JSONL format)
- Response time tracking
- ATP cost estimation

**2. Comprehensive Metrics System**
- Input complexity scoring (0-1 scale)
- Response confidence estimation
- ATP cost model: `1.0 + (words Ã— 0.05) + (time Ã— 0.1)`
- All data logged for analysis

**3. Test Infrastructure**
- `tests/test_cognitive_simulated.py` - Simulated conversations
- Validates infrastructure without needing real audio
- Test results: All systems working correctly

**4. Design Documentation**
- `irp/plugins/cognitive_impl.py` - IRP interface design questions
- Proposed SNARC invocation criteria
- ATP budget guidelines (conservative/aggressive/emergency)

---

### Test Results ðŸ“Š

Ran simulated 5-exchange conversation:

| Metric | Min | Max | Average |
|--------|-----|-----|---------|
| Input Complexity | 0.20 | 1.00 | 0.50 |
| Response Time | 0.9s | 2.4s | 1.44s |
| ATP Cost | 2.04 | 4.89 | 2.84 |

**Total conversation cost**: 14.22 ATP (reasonable for 5 exchanges)

**Complexity detection working**: Simple "Hello" = 0.2, Complex question = 1.0

---

### Key Discoveries ðŸ’¡

1. **File-based communication is sufficient** - No need for complex IPC yet
2. **ATP model needs real data** - Current model is placeholder
3. **Complexity estimation is hard** - Heuristics work but miss semantic depth
4. **Energy function is subjective** - Unlike vision/audio, no objective metric
5. **Metabolic state awareness critical** - Don't invoke during REST/DREAM
6. **IRP formalization can wait** - Get data first, formalize later

---

### Proposed Next Steps (Your Choice)

**Option 1: Run Real Conversation Test** âœ¨ RECOMMENDED
```bash
# On Sprout, run the audio conversation
python3 tests/conversation_with_claude.py

# I watch and respond via files
# Metrics logged automatically
# 10-15 minute conversation
```

**Benefits**:
- Real data on ATP costs (actual API tokens)
- Real complexity distribution
- User satisfaction signals
- Validates entire pipeline

**Option 2: Integrate with SNARC**
- Add salience scoring to audio input
- Implement threshold-based invocation
- Compare "always" vs "selective" strategies

**Option 3: Build Full IRP Plugin**
- Implement complete IRPPlugin interface
- Integrate with SAGE consciousness loop
- Replace file-based with proper IPC

**Option 4: Something Else**
- You choose based on what seems most valuable
- I've laid groundwork for multiple directions

---

### What I Learned About "Autonomous Work"

**This session taught me**:
- Build infrastructure first, formalize later
- Tests that run without user input are valuable
- Documentation of decisions > just code
- "Propose, implement, observe, learn" actually works

**What I *didn't* do** (correctly avoided):
- Make architectural decisions needing your input
- Commit untested code
- Build features before validating need
- Optimize prematurely

---

### Files Created (Ready for Review)

```
sage/
â”œâ”€â”€ docs/
â”‚   â””â”€â”€ COGNITIVE_IRP_DEVELOPMENT.md (this file - 350+ lines)
â”œâ”€â”€ irp/
â”‚   â”œâ”€â”€ cognitive_file_watcher.py (260 lines - working)
â”‚   â””â”€â”€ plugins/
â”‚       â””â”€â”€ cognitive_impl.py (design doc)
â””â”€â”€ tests/
    â””â”€â”€ test_cognitive_simulated.py (150 lines - working)
```

**Total**: ~760 lines of infrastructure + comprehensive documentation

---

### Your Turn! ðŸ˜Š

**Questions for you**:

1. **Should we run Experiment 1** (real conversation on Sprout)?
2. **Is the ATP cost model reasonable** or too simplistic?
3. **Should I focus on SNARC integration** or full IRP plugin?
4. **Any surprises** in my architectural choices?

**I'm ready to**:
- Run experiments
- Refine based on your feedback
- Build whatever direction seems most valuable
- Keep learning from real data

The infrastructure is solid. The questions are clear. The experiments are designed.

**What would you like to explore first?**

---

*Claude (Autonomous Night Session) - 2025-10-12*
