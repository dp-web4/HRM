#!/usr/bin/env python3
"""
Hierarchical Audio Buffer - Memory-Efficient Audio Management

Replaces disk-based audio storage with intelligent tiered buffering:
- Tier 1: Rolling capture buffer (5s, always in memory)
- Tier 2: Speech segments (VAD-detected, awaiting transcription)
- Tier 3: Transcriptions only (audio discarded)

Storage: 19GB/day â†’ 3.4MB maximum (99.98% reduction)
"""

import numpy as np
import threading
import time
from collections import deque
from typing import Optional, Dict, Any, List, Tuple
from dataclasses import dataclass
from datetime import datetime


@dataclass
class SpeechSegment:
    """A segment of detected speech awaiting transcription"""
    audio: np.ndarray  # Raw audio samples
    timestamp: float
    duration: float
    salience: Optional[float] = None  # SNARC salience score (future)
    metadata: Dict[str, Any] = None

    def __post_init__(self):
        if self.metadata is None:
            self.metadata = {}


class HierarchicalAudioBuffer:
    """
    Three-tier audio buffer for intelligent memory management.

    Tier 1: Rolling Capture (Circular Buffer)
    - Fixed 5-second window
    - Constantly overwriting oldest
    - Captures everything, retains nothing permanently

    Tier 2: Speech Segments (Queue)
    - Up to 10 detected speech segments
    - Awaiting transcription
    - SNARC-based eviction (future)

    Tier 3: Transcriptions (Text Storage)
    - Only the transcribed text
    - Audio discarded after transcription
    - Permanent storage in SNARC memory
    """

    def __init__(
        self,
        sample_rate: int = 16000,
        rolling_duration: float = 5.0,
        max_speech_segments: int = 10,
        frame_duration_ms: int = 30
    ):
        """
        Initialize hierarchical audio buffer.

        Args:
            sample_rate: Audio sample rate (Hz)
            rolling_duration: Duration of rolling capture buffer (seconds)
            max_speech_segments: Maximum speech segments to retain
            frame_duration_ms: Frame duration in milliseconds
        """
        self.sample_rate = sample_rate
        self.rolling_duration = rolling_duration
        self.max_speech_segments = max_speech_segments
        self.frame_duration_ms = frame_duration_ms

        # Calculate buffer sizes
        self.frame_size = int(sample_rate * frame_duration_ms / 1000)
        self.rolling_capacity = int(rolling_duration * sample_rate / self.frame_size)

        # Tier 1: Rolling capture buffer (circular)
        self.rolling_buffer = deque(maxlen=self.rolling_capacity)
        self.rolling_lock = threading.Lock()

        # Tier 2: Speech segments queue
        self.speech_segments: List[SpeechSegment] = []
        self.speech_lock = threading.Lock()

        # Stats
        self.stats = {
            'total_frames': 0,
            'speech_segments_captured': 0,
            'speech_segments_evicted': 0,
            'transcriptions_completed': 0,
            'bytes_saved_vs_disk': 0
        }

        print(f"ðŸ“¼ Hierarchical Audio Buffer initialized")
        print(f"   Tier 1 (Rolling): {rolling_duration}s = {self.rolling_capacity} frames")
        print(f"   Tier 2 (Speech): {max_speech_segments} segments max")
        print(f"   Frame size: {self.frame_size} samples ({frame_duration_ms}ms)")

    # =========================================================================
    # Tier 1: Rolling Capture Buffer
    # =========================================================================

    def push_frame(self, audio_frame: np.ndarray) -> None:
        """
        Push audio frame to rolling capture buffer.

        This is called continuously for ALL audio. Oldest frames are
        automatically overwritten when buffer is full.

        Args:
            audio_frame: Audio samples (numpy array)
        """
        with self.rolling_lock:
            self.rolling_buffer.append(audio_frame)
            self.stats['total_frames'] += 1

    def get_rolling_buffer(self, duration_seconds: Optional[float] = None) -> np.ndarray:
        """
        Get audio from rolling capture buffer.

        Args:
            duration_seconds: How many seconds to retrieve (None = all)

        Returns:
            Numpy array of audio samples
        """
        with self.rolling_lock:
            if not self.rolling_buffer:
                return np.array([], dtype=np.int16)

            if duration_seconds is None:
                # Return entire buffer
                frames = list(self.rolling_buffer)
            else:
                # Calculate how many frames to retrieve
                num_frames = int(duration_seconds * self.sample_rate / self.frame_size)
                num_frames = min(num_frames, len(self.rolling_buffer))
                # Get most recent N frames
                frames = list(self.rolling_buffer)[-num_frames:]

            # Concatenate all frames
            return np.concatenate(frames) if frames else np.array([], dtype=np.int16)

    def get_rolling_duration(self) -> float:
        """Get current duration of audio in rolling buffer (seconds)."""
        with self.rolling_lock:
            num_frames = len(self.rolling_buffer)
            return num_frames * self.frame_size / self.sample_rate

    # =========================================================================
    # Tier 2: Speech Segments
    # =========================================================================

    def promote_to_speech(self, duration_seconds: float = None, metadata: Dict[str, Any] = None) -> SpeechSegment:
        """
        Promote rolling buffer content to speech segment queue.

        Called when VAD detects speech. Copies audio from rolling buffer
        to speech segment for transcription.

        Args:
            duration_seconds: How much audio to capture (default: all)
            metadata: Additional metadata for this segment

        Returns:
            Created SpeechSegment
        """
        # Get audio from rolling buffer
        audio = self.get_rolling_buffer(duration_seconds)

        if len(audio) == 0:
            raise ValueError("No audio in rolling buffer to promote")

        # Create speech segment
        segment = SpeechSegment(
            audio=audio,
            timestamp=time.time(),
            duration=len(audio) / self.sample_rate,
            metadata=metadata or {}
        )

        with self.speech_lock:
            # Add to queue
            self.speech_segments.append(segment)
            self.stats['speech_segments_captured'] += 1

            # Evict oldest if over capacity (FIFO for now, SNARC-based later)
            while len(self.speech_segments) > self.max_speech_segments:
                evicted = self.speech_segments.pop(0)
                self.stats['speech_segments_evicted'] += 1
                # Calculate how much disk space we saved
                self.stats['bytes_saved_vs_disk'] += len(evicted.audio) * 2  # 16-bit = 2 bytes

        return segment

    def get_next_speech_segment(self) -> Optional[SpeechSegment]:
        """
        Get next speech segment for transcription (FIFO).

        Returns:
            SpeechSegment or None if queue is empty
        """
        with self.speech_lock:
            if not self.speech_segments:
                return None
            return self.speech_segments.pop(0)

    def get_speech_segment_count(self) -> int:
        """Get number of speech segments waiting for transcription."""
        with self.speech_lock:
            return len(self.speech_segments)

    # =========================================================================
    # Tier 3: Transcription
    # =========================================================================

    def mark_transcription_complete(self, segment: SpeechSegment, transcription: str) -> None:
        """
        Mark segment as transcribed. Audio is discarded.

        Args:
            segment: The speech segment that was transcribed
            transcription: The text transcription
        """
        # Calculate storage savings
        audio_bytes = len(segment.audio) * 2  # 16-bit samples
        text_bytes = len(transcription.encode('utf-8'))
        savings = audio_bytes - text_bytes

        self.stats['transcriptions_completed'] += 1
        self.stats['bytes_saved_vs_disk'] += savings

        # Audio is garbage collected automatically when segment is discarded
        # Transcription should be stored in SNARC memory (handled by caller)

    # =========================================================================
    # Statistics and Monitoring
    # =========================================================================

    def get_stats(self) -> Dict[str, Any]:
        """Get buffer statistics."""
        with self.rolling_lock:
            rolling_frames = len(self.rolling_buffer)
            rolling_duration = self.get_rolling_duration()

        with self.speech_lock:
            speech_count = len(self.speech_segments)
            speech_bytes = sum(len(s.audio) * 2 for s in self.speech_segments)

        return {
            **self.stats,
            'rolling_buffer': {
                'frames': rolling_frames,
                'capacity': self.rolling_capacity,
                'utilization': rolling_frames / self.rolling_capacity if self.rolling_capacity > 0 else 0,
                'duration_seconds': rolling_duration,
                'memory_bytes': rolling_frames * self.frame_size * 2  # 16-bit
            },
            'speech_queue': {
                'segments': speech_count,
                'capacity': self.max_speech_segments,
                'utilization': speech_count / self.max_speech_segments if self.max_speech_segments > 0 else 0,
                'memory_bytes': speech_bytes
            },
            'storage_efficiency': {
                'bytes_saved': self.stats['bytes_saved_vs_disk'],
                'mb_saved': self.stats['bytes_saved_vs_disk'] / (1024 * 1024)
            }
        }

    def print_stats(self) -> None:
        """Print buffer statistics (for debugging)."""
        stats = self.get_stats()

        print("\nðŸ“Š Hierarchical Audio Buffer Stats:")
        print(f"   Total frames: {stats['total_frames']}")
        print(f"\n   Tier 1 (Rolling):")
        print(f"      Frames: {stats['rolling_buffer']['frames']}/{stats['rolling_buffer']['capacity']}")
        print(f"      Duration: {stats['rolling_buffer']['duration_seconds']:.2f}s")
        print(f"      Memory: {stats['rolling_buffer']['memory_bytes'] / 1024:.1f} KB")
        print(f"\n   Tier 2 (Speech):")
        print(f"      Segments: {stats['speech_queue']['segments']}/{stats['speech_queue']['capacity']}")
        print(f"      Memory: {stats['speech_queue']['memory_bytes'] / 1024:.1f} KB")
        print(f"\n   Tier 3 (Transcription):")
        print(f"      Completed: {stats['transcriptions_completed']}")
        print(f"      Evicted: {stats['speech_segments_evicted']}")
        print(f"\n   Storage Efficiency:")
        print(f"      Saved vs disk: {stats['storage_efficiency']['mb_saved']:.2f} MB")


# ============================================================================
# Example Usage
# ============================================================================

if __name__ == "__main__":
    # Create buffer
    buffer = HierarchicalAudioBuffer(
        sample_rate=16000,
        rolling_duration=5.0,
        max_speech_segments=10
    )

    # Simulate continuous audio capture
    print("\nSimulating continuous audio capture...")
    for i in range(200):  # 200 frames = 6 seconds
        # Generate fake audio frame (30ms at 16kHz)
        fake_frame = np.random.randint(-32768, 32767, size=480, dtype=np.int16)
        buffer.push_frame(fake_frame)

        # Simulate speech detection at frame 100
        if i == 100:
            print("   [VAD] Speech detected! Promoting to Tier 2...")
            segment = buffer.promote_to_speech(duration_seconds=2.0)
            print(f"   [TIER 2] Captured {segment.duration:.2f}s segment")

        # Simulate transcription at frame 150
        if i == 150:
            print("   [TRANSCRIPTION] Processing speech segment...")
            segment = buffer.get_next_speech_segment()
            if segment:
                transcription = "Hello, this is a test transcription."
                buffer.mark_transcription_complete(segment, transcription)
                print(f"   [TIER 3] Transcribed: '{transcription}'")
                print(f"   [TIER 3] Audio discarded, saved {len(segment.audio)*2} bytes")

    # Print final stats
    buffer.print_stats()

    print("\nâœ… Hierarchical buffer demonstration complete!")
    print("   No files written to disk - everything in memory!")
