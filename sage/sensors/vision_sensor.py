#!/usr/bin/env python3
"""
Vision Sensor for SAGE Consciousness - Track 4
===============================================

Real-time camera integration with cognitive architecture.

Integrates:
- CSI/USB camera capture (from existing proven code)
- Track 1: Sensor trust tracking
- Track 2: Memory storage of visual observations
- Track 3: Attention-guided processing, cognitive integration

Based on existing sage/irp/plugins/camera_sensor_impl.py (proven CSI code)
Extends with Track 1-3 integration per Track 4 architecture.
"""

import cv2
import torch
import numpy as np
from typing import Optional, Dict, Any, Tuple, List
from dataclasses import dataclass, field
from enum import Enum
import time
import threading
import queue


@dataclass
class VisionObservation:
    """
    Vision sensor observation

    Integrates with Track 1-3:
    - frame: Raw camera data
    - salience: For Track 3 attention allocation
    - features: For Track 2 memory storage
    - trust_score: For Track 1 sensor fusion
    """
    frame: np.ndarray  # Raw frame [H, W, 3] RGB
    timestamp: float
    salience: float  # Aggregate salience score (0-1)
    features: Optional[torch.Tensor] = None  # Encoded features for memory
    metadata: Dict[str, Any] = field(default_factory=dict)
    trust_score: float = 1.0  # From Track 1 sensor trust


class CameraBackend(Enum):
    """Camera backend types"""
    CSI = "csi"  # CSI camera (Jetson Nano)
    USB = "usb"  # USB webcam
    SIMULATED = "simulated"  # Test/fallback mode


class VisionSensor:
    """
    Real-time vision sensor for SAGE consciousness

    Architecture (Track 4):
    1. Camera capture (CSI/USB/Simulated)
    2. Preprocessing (resize, normalize)
    3. Salience computation (novelty detection)
    4. Feature extraction (for memory)

    Integration:
    - Track 1: Reports trust score based on consistency
    - Track 2: Stores observations in memory
    - Track 3: Provides salience for attention allocation

    Based on proven CSI implementation from sage/irp/plugins/camera_sensor_impl.py
    """

    def __init__(
        self,
        backend: str = "auto",
        sensor_id: int = 0,
        resolution: Tuple[int, int] = (1920, 1080),
        display_resolution: Tuple[int, int] = (640, 480),
        fps: int = 30,
        device: str = "cuda",
        sensor_trust=None,  # Track 1 integration
        memory_system=None,  # Track 2 integration
        attention_manager=None  # Track 3 integration
    ):
        """
        Initialize vision sensor

        Args:
            backend: 'auto', 'csi', 'usb', or 'simulated'
            sensor_id: Camera ID (0, 1 for CSI or USB ID)
            resolution: Capture resolution (W, H)
            display_resolution: Processing resolution (W, H)
            fps: Target frame rate
            device: 'cuda' or 'cpu'
            sensor_trust: Track 1 SensorTrust instance
            memory_system: Track 2 Memory instance
            attention_manager: Track 3 AttentionManager instance
        """
        self.sensor_id = sensor_id
        self.resolution = resolution
        self.display_resolution = display_resolution
        self.fps = fps
        self.device = torch.device(device if torch.cuda.is_available() else 'cpu')

        # Track 1-3 integration
        self.sensor_trust = sensor_trust
        self.memory = memory_system
        self.attention = attention_manager

        # Select backend
        self.backend = self._detect_backend(backend)
        print(f"ðŸ“· Vision sensor: {self.backend.value} (ID {sensor_id}) @ {fps}fps")

        # Camera capture
        self.cap = None
        self.capture_thread = None
        self.running = False
        self.frame_queue = queue.Queue(maxsize=2)
        self.latest_frame = None

        # Performance tracking
        self.frame_count = 0
        self.start_time = time.time()
        self.total_latency = 0.0
        self.fps_actual = 0.0

        # Salience baseline (for novelty detection)
        self.frame_history = []
        self.history_size = 30  # 1 second at 30 FPS

        # Initialize camera
        self._initialize_camera()

    def _detect_backend(self, backend: str) -> CameraBackend:
        """Detect or select camera backend"""
        if backend == "auto":
            # Auto-detect: CSI > USB > Simulated
            if self._is_jetson():
                return CameraBackend.CSI
            elif self._has_usb_camera():
                return CameraBackend.USB
            else:
                print("âš ï¸  No camera found, using simulated mode")
                return CameraBackend.SIMULATED

        backend_map = {
            "csi": CameraBackend.CSI,
            "usb": CameraBackend.USB,
            "simulated": CameraBackend.SIMULATED
        }
        return backend_map.get(backend, CameraBackend.SIMULATED)

    def _is_jetson(self) -> bool:
        """Check if running on Jetson platform"""
        try:
            with open('/proc/device-tree/model', 'r') as f:
                model = f.read()
                return 'jetson' in model.lower()
        except FileNotFoundError:
            return False

    def _has_usb_camera(self) -> bool:
        """Check if USB camera is available"""
        try:
            cap = cv2.VideoCapture(self.sensor_id)
            if cap.isOpened():
                cap.release()
                return True
        except:
            pass
        return False

    def _initialize_camera(self):
        """Initialize camera based on backend"""
        if self.backend == CameraBackend.CSI:
            # CSI camera with GStreamer (from proven implementation)
            pipeline = self._create_gst_pipeline()
            self.cap = cv2.VideoCapture(pipeline, cv2.CAP_GSTREAMER)
            print(f"  CSI pipeline: nvarguscamerasrc sensor-id={self.sensor_id}")

        elif self.backend == CameraBackend.USB:
            # USB camera
            self.cap = cv2.VideoCapture(self.sensor_id)
            if self.cap.isOpened():
                self.cap.set(cv2.CAP_PROP_FRAME_WIDTH, self.resolution[0])
                self.cap.set(cv2.CAP_PROP_FRAME_HEIGHT, self.resolution[1])
                self.cap.set(cv2.CAP_PROP_FPS, self.fps)
                self.cap.set(cv2.CAP_PROP_BUFFERSIZE, 1)  # Minimize latency
            print(f"  USB camera {self.sensor_id} opened")

        elif self.backend == CameraBackend.SIMULATED:
            # Simulated mode (no actual camera)
            print("  Simulated camera mode (synthetic frames)")

        # Verify camera opened
        if self.backend != CameraBackend.SIMULATED:
            if not self.cap or not self.cap.isOpened():
                raise RuntimeError(f"Failed to open camera (backend: {self.backend.value})")

        # Register with Track 1 sensor trust
        if self.sensor_trust:
            self.sensor_trust.register_sensor(
                sensor_id='vision',
                sensor_type='camera',
                initial_trust=0.5,  # Neutral start
                decay_rate=0.01
            )
            print("  âœ“ Registered with Track 1 (Sensor Trust)")

        # Start capture thread
        self.running = True
        self.capture_thread = threading.Thread(target=self._capture_loop, daemon=True)
        self.capture_thread.start()

        print(f"âœ“ Vision sensor initialized: {self.backend.value}")

    def _create_gst_pipeline(self) -> str:
        """
        Create GStreamer pipeline for CSI camera

        From proven implementation in sage/irp/plugins/camera_sensor_impl.py
        Uses sensor-mode=2 for 1920x1080 @ 30fps
        """
        width, height = self.resolution
        disp_width, disp_height = self.display_resolution

        return (
            f"nvarguscamerasrc sensor-id={self.sensor_id} sensor-mode=2 ! "
            f"video/x-raw(memory:NVMM), width={width}, height={height}, "
            f"format=NV12, framerate={self.fps}/1 ! "
            f"nvvidconv ! video/x-raw, width={disp_width}, "
            f"height={disp_height}, format=BGRx ! "
            f"videoconvert ! video/x-raw, format=BGR ! "
            f"appsink drop=true max-buffers=1 sync=false"
        )

    def _capture_loop(self):
        """Background thread for continuous frame capture"""
        while self.running:
            if self.backend == CameraBackend.SIMULATED:
                # Generate synthetic frame
                time.sleep(1.0 / self.fps)
                frame = self._generate_synthetic_frame()
            else:
                # Capture from camera
                if self.cap and self.cap.isOpened():
                    ret, frame = self.cap.read()
                    if not ret:
                        continue
                else:
                    time.sleep(0.1)
                    continue

            self.latest_frame = frame
            self.frame_count += 1

            # Update FPS
            elapsed = time.time() - self.start_time
            if elapsed > 0:
                self.fps_actual = self.frame_count / elapsed

            # Queue for processing
            try:
                self.frame_queue.put_nowait(frame)
            except queue.Full:
                # Drop oldest frame
                try:
                    self.frame_queue.get_nowait()
                    self.frame_queue.put_nowait(frame)
                except queue.Empty:
                    pass

    def _generate_synthetic_frame(self) -> np.ndarray:
        """Generate synthetic frame for testing"""
        # Create colored noise pattern
        frame = np.random.randint(0, 255, (*self.display_resolution[::-1], 3), dtype=np.uint8)
        # Add some structure (moving pattern)
        offset = self.frame_count % 100
        frame[offset:offset+50, :] = [255, 0, 0]  # Red moving bar
        return frame

    def capture(self) -> Optional[VisionObservation]:
        """
        Capture and process frame

        Returns:
            VisionObservation with frame, salience, features, trust score
        """
        start_time = time.time()

        # Get latest frame
        try:
            frame = self.frame_queue.get(timeout=0.1)
        except queue.Empty:
            frame = self.latest_frame

        if frame is None:
            return None

        # Convert BGR to RGB (OpenCV captures in BGR)
        if self.backend != CameraBackend.SIMULATED:
            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        else:
            frame_rgb = frame

        # Compute salience (simple novelty detection for now)
        salience = self._compute_salience(frame_rgb)

        # Extract features (placeholder - would use SNARC encoder from Track 2)
        features = self._extract_features(frame_rgb)

        # Compute trust score
        trust_score = self._compute_trust_score()

        # Create observation
        latency = time.time() - start_time
        self.total_latency += latency

        observation = VisionObservation(
            frame=frame_rgb,
            timestamp=time.time(),
            salience=salience,
            features=features,
            trust_score=trust_score,
            metadata={
                'latency_ms': latency * 1000,
                'frame_count': self.frame_count,
                'fps': self.fps_actual,
                'backend': self.backend.value,
                'resolution': self.display_resolution
            }
        )

        # Update Track 1 sensor trust
        if self.sensor_trust:
            self.sensor_trust.update('vision', trust_score)

        # Store in Track 2 memory (if high salience)
        if self.memory and salience > 0.5:
            # TODO: Store observation in memory
            pass

        return observation

    def _compute_salience(self, frame: np.ndarray) -> float:
        """
        Compute salience score (0-1)

        Simple novelty detection: compare to recent frame history
        High novelty = high salience
        """
        # Add to history
        self.frame_history.append(frame.copy())
        if len(self.frame_history) > self.history_size:
            self.frame_history.pop(0)

        if len(self.frame_history) < 2:
            return 0.5  # Neutral salience initially

        # Compute novelty as difference from recent average
        recent_avg = np.mean(self.frame_history[-5:], axis=0)
        diff = np.abs(frame.astype(float) - recent_avg)
        novelty = np.mean(diff) / 255.0  # Normalize to 0-1

        # Map novelty to salience (0-1 range)
        salience = np.clip(novelty * 5.0, 0.0, 1.0)  # Scale up novelty

        return float(salience)

    def _extract_features(self, frame: np.ndarray) -> Optional[torch.Tensor]:
        """
        Extract visual features

        Placeholder: Would use SNARC vision encoder from Track 2
        For now, return simple features
        """
        # Downsample and convert to tensor
        small = cv2.resize(frame, (32, 32))
        tensor = torch.from_numpy(small).to(self.device).float() / 255.0
        tensor = tensor.permute(2, 0, 1).flatten()  # [C, H, W] -> [C*H*W]
        return tensor

    def _compute_trust_score(self) -> float:
        """
        Compute sensor trust score

        Criteria:
        - Latency consistency (low variance = high trust)
        - Frame rate consistency (meeting target FPS = high trust)
        - No corruption (all frames valid = high trust)
        """
        if self.frame_count < 10:
            return 0.5  # Neutral trust initially

        # FPS consistency
        fps_target_ratio = min(self.fps_actual / self.fps, 1.0) if self.fps_actual > 0 else 0.0
        fps_trust = fps_target_ratio

        # Latency consistency (avg latency < 50ms = good)
        avg_latency = self.total_latency / self.frame_count if self.frame_count > 0 else 0.1
        latency_trust = 1.0 / (1.0 + avg_latency * 10)  # Penalize high latency

        # Combine
        trust = (fps_trust * 0.5 + latency_trust * 0.5)
        return float(np.clip(trust, 0.0, 1.0))

    def get_stats(self) -> Dict[str, Any]:
        """Get vision sensor statistics"""
        avg_latency = self.total_latency / self.frame_count if self.frame_count > 0 else 0.0

        return {
            'backend': self.backend.value,
            'sensor_id': self.sensor_id,
            'frame_count': self.frame_count,
            'fps_target': self.fps,
            'fps_actual': round(self.fps_actual, 2),
            'avg_latency_ms': round(avg_latency * 1000, 2),
            'resolution': self.display_resolution,
            'trust_score': round(self._compute_trust_score(), 3)
        }

    def shutdown(self):
        """Shutdown vision sensor"""
        print(f"Shutting down vision sensor...")
        self.running = False

        if self.capture_thread:
            self.capture_thread.join(timeout=2.0)

        if self.cap:
            self.cap.release()

        print("âœ“ Vision sensor shutdown complete")


def test_vision_sensor():
    """Test vision sensor in simulated mode"""
    print("\n" + "="*60)
    print("TESTING VISION SENSOR (Track 4)")
    print("="*60)

    # Create sensor in simulated mode
    sensor = VisionSensor(
        backend="simulated",
        fps=30,
        device="cuda" if torch.cuda.is_available() else "cpu"
    )

    print("\n1. Capturing frames...")
    observations = []
    for i in range(10):
        obs = sensor.capture()
        if obs:
            observations.append(obs)
            print(f"   Frame {i+1}: salience={obs.salience:.3f}, "
                  f"latency={obs.metadata['latency_ms']:.2f}ms, "
                  f"trust={obs.trust_score:.3f}")
        time.sleep(0.033)  # ~30 FPS

    print(f"\n2. Captured {len(observations)} frames")

    # Check stats
    stats = sensor.get_stats()
    print(f"\n3. Statistics:")
    print(f"   Backend: {stats['backend']}")
    print(f"   FPS: {stats['fps_actual']:.1f} (target {stats['fps_target']})")
    print(f"   Avg latency: {stats['avg_latency_ms']:.2f}ms")
    print(f"   Trust score: {stats['trust_score']:.3f}")
    print(f"   Resolution: {stats['resolution']}")

    # Test salience computation
    saliences = [obs.salience for obs in observations]
    print(f"\n4. Salience range: {min(saliences):.3f} - {max(saliences):.3f}")
    print(f"   Mean salience: {np.mean(saliences):.3f}")

    # Shutdown
    sensor.shutdown()

    print("\n" + "="*60)
    print("âœ… VISION SENSOR TEST PASSED")
    print("="*60)
    print("\nReady for:")
    print("  âœ“ CSI camera testing on Nano")
    print("  âœ“ Track 1 (Sensor Trust) integration")
    print("  âœ“ Track 2 (Memory) integration")
    print("  âœ“ Track 3 (Attention/Cognition) integration")


if __name__ == "__main__":
    test_vision_sensor()
