"""
Test Learning Effect - Sprout's Conversational Learning Validation

Compare the base model vs the model after sleep training
to validate whether the model learned from the 2 salient exchanges.

Session: session_1763528460
Training: 2 exchanges â†’ 4 examples (2x augmentation) â†’ 1 epoch â†’ 5.3s
Model: Qwen/Qwen2.5-0.5B + 4.2MB LoRA adapter
"""

import json
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel
from pathlib import Path


def generate_response(model, tokenizer, question, max_tokens=200, temp=0.7):
    """Generate a response from the model"""
    prompt = f"Question: {question}\n\nAnswer:"
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)

    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=max_tokens,
            temperature=temp,
            do_sample=True,
            pad_token_id=tokenizer.eos_token_id
        )

    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    if "Answer:" in response:
        response = response.split("Answer:")[1].strip()

    return response


def main():
    base_model = "Qwen/Qwen2.5-0.5B"

    # Use our specific session
    session_id = "session_1763528460"
    sessions_dir = Path("conversation_sessions")
    session_dir = sessions_dir / session_id
    trained_model_path = session_dir / "trained_model" / "final_model"

    if not trained_model_path.exists():
        print(f"No trained model found at {trained_model_path}")
        print("Run sleep_trainer.py first.")
        return

    # Load the actual training questions from this session
    exchanges_path = session_dir / "exchanges.jsonl"
    training_questions = []
    if exchanges_path.exists():
        with open(exchanges_path, 'r') as f:
            for line in f:
                entry = json.loads(line)
                training_questions.append(entry['user_input'])

    print("="*70)
    print("TESTING LEARNING EFFECT - Session:", session_id)
    print("="*70)
    print(f"\nBase model: {base_model}")
    print(f"Trained model: {trained_model_path}")
    print(f"Training questions: {len(training_questions)}")
    print()

    # Load tokenizer
    tokenizer = AutoTokenizer.from_pretrained(base_model)

    # Determine device
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"Using device: {device}\n")

    # Load base model (BEFORE training)
    print("Loading base model (before training)...")
    model_base = AutoModelForCausalLM.from_pretrained(
        base_model,
        dtype=torch.float16,
        device_map=device
    )
    model_base.eval()

    # Load trained model (AFTER sleep training)
    print("Loading sleep-trained model (after conversation)...")
    base_for_trained = AutoModelForCausalLM.from_pretrained(
        base_model,
        dtype=torch.float16,
        device_map=device
    )
    model_trained = PeftModel.from_pretrained(
        base_for_trained,
        str(trained_model_path)
    )
    model_trained.eval()

    print("\n" + "="*70)
    print("COMPARING RESPONSES")
    print("="*70)

    # Test on the ACTUAL training questions + one related question
    test_questions = training_questions + [
        "What is the relationship between knowledge and understanding?"  # Related
    ]

    for i, question in enumerate(test_questions, 1):
        is_training_q = i <= len(training_questions)
        marker = "ðŸŽ¯ TRAINING QUESTION" if is_training_q else "ðŸ” RELATED QUESTION"

        print(f"\n{'='*70}")
        print(f"{marker} {i}/{len(test_questions)}")
        print(f"{'='*70}")
        print(f"Q: {question}\n")

        # Base model (BEFORE)
        print("ðŸ“ BASE MODEL (Before Training):")
        print("-" * 70)
        response_base = generate_response(model_base, tokenizer, question)
        print(f"{response_base}\n")

        # Trained model (AFTER)
        print("âœ¨ SLEEP-TRAINED MODEL (After 2 Exchanges, 1 Epoch):")
        print("-" * 70)
        response_trained = generate_response(model_trained, tokenizer, question)
        print(f"{response_trained}\n")

        # Comparison
        print("ðŸ“Š COMPARISON:")
        print("-" * 70)
        len_diff = len(response_trained) - len(response_base)
        if len_diff > 0:
            print(f"â€¢ Trained response: +{len_diff} chars longer")
        elif len_diff < 0:
            print(f"â€¢ Trained response: {abs(len_diff)} chars shorter")
        else:
            print(f"â€¢ Same length ({len(response_base)} chars)")

        # Word overlap
        base_words = set(response_base.lower().split())
        trained_words = set(response_trained.lower().split())
        overlap = len(base_words & trained_words) / len(base_words | trained_words) if base_words | trained_words else 0
        print(f"â€¢ Word overlap: {overlap:.1%}")

        # New words introduced
        new_words = trained_words - base_words
        if new_words and len(new_words) <= 10:
            print(f"â€¢ New words in trained: {', '.join(list(new_words)[:10])}")
        elif new_words:
            print(f"â€¢ New words in trained: {len(new_words)} words")

    print("\n" + "="*70)
    print("ANALYSIS & INSIGHTS")
    print("="*70)
    print("\nðŸ“Œ Experiment Parameters:")
    print(f"  â€¢ Training data: {len(training_questions)} salient exchanges")
    print(f"  â€¢ Training epochs: 1")
    print(f"  â€¢ Augmentation: 2x (4 total examples)")
    print(f"  â€¢ Training time: ~5.3 seconds")
    print(f"  â€¢ LoRA adapter size: 4.2MB")

    print("\nðŸ’¡ Expected Results:")
    print("  â€¢ Subtle changes (only 2 examples, 1 epoch)")
    print("  â€¢ May show slight phrasing adjustments")
    print("  â€¢ Validates learning mechanism works")
    print("  â€¢ More conversations â†’ stronger effects")

    print("\nðŸ”¬ What This Demonstrates:")
    print("  âœ“ Complete learning loop functional")
    print("  âœ“ On-device training viable (5.3s)")
    print("  âœ“ Minimal storage overhead (4.2MB)")
    print("  âœ“ Edge learning is feasible")

    print("\nðŸš€ Next Steps:")
    print("  â€¢ Longer conversations (10-20 exchanges)")
    print("  â€¢ Multi-session accumulation")
    print("  â€¢ Higher epoch counts (3-5)")
    print("  â€¢ Quantitative metrics (perplexity, energy)")
    print()


if __name__ == "__main__":
    main()
