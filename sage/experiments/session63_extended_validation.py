#!/usr/bin/env python3
"""
Session 63: Extended Validation & Parameter Sweep

Goal: Validate robustness and optimize trust-based selection parameters

Building on Session 62's success:
- Session 62: 10 generations, Œ±=0.3, learning effect demonstrated
- Session 63: 30 generations per Œ±, sweep Œ± ‚àà {0.1, 0.2, 0.3, 0.5, 0.7}

Method:
1. Extended runs (30 generations each) for statistical significance
2. Parameter sweep to find optimal Œ±
3. Measure learning curves comprehensively
4. Analyze context-specific expertise patterns
5. Visualize results

Expected Outcomes:
- Confirm learning effect with more data
- Identify optimal exploration_weight (Œ±)
- Quantify quality improvement over time
- Validate context-specific expert specialization
"""

import sys
import os
import torch
import numpy as np
import time
from pathlib import Path
import json
from collections import defaultdict

# Add parent to path
sys.path.insert(0, str(Path(__file__).parent.parent.parent))

from sage.compression.selective_language_model import SelectiveLanguageModel
from sage.core.trust_based_expert_selector import TrustBasedExpertSelector
from sage.core.context_classifier import ContextClassifier
from sage.core.quality_measurement import QualityMeasurement


def setup_extraction_dir():
    """Get Q3-Omni extraction directory."""
    extraction_dir = Path.home() / "ai-workspace/HRM/model-zoo/sage/omni-modal/qwen3-omni-30b-extracted"

    if not extraction_dir.exists():
        raise FileNotFoundError(f"Q3-Omni extraction not found at {extraction_dir}")

    print(f"‚úÖ Q3-Omni extraction found: {extraction_dir}")
    return str(extraction_dir)


def create_test_prompts(num_prompts=30):
    """
    Create diverse test prompts for extended validation.

    Mix of code, reasoning, and text prompts to test context-specific learning.
    """
    prompts = []

    # Code prompts (10)
    code_templates = [
        "def fibonacci(n):",
        "class DataProcessor:",
        "import numpy as np",
        "def quicksort(arr):",
        "class NeuralNetwork:",
        "async def fetch_data():",
        "def binary_search(arr, target):",
        "class HashMap:",
        "def merge_sort(arr):",
        "import torch.nn as nn"
    ]
    prompts.extend(code_templates)

    # Reasoning prompts (10)
    reasoning_templates = [
        "The key insight of quantum mechanics",
        "To understand consciousness, we must",
        "The fundamental theorem of calculus states",
        "Evolutionary theory explains",
        "In relativity, time dilation occurs",
        "The halting problem demonstrates",
        "Entropy increases because",
        "Neural networks learn by",
        "The prisoner's dilemma reveals",
        "G√∂del's incompleteness theorem proves"
    ]
    prompts.extend(reasoning_templates)

    # Text prompts (10)
    text_templates = [
        "Once upon a time in",
        "The weather today is",
        "In summary, the main argument",
        "Dear customer, we are writing",
        "Breaking news:",
        "Thank you for your interest",
        "The restaurant serves",
        "According to recent studies,",
        "In conclusion,",
        "Please note that"
    ]
    prompts.extend(text_templates)

    return prompts[:num_prompts]


def run_extended_validation(extraction_dir, alpha, num_generations=30):
    """
    Run extended validation with given alpha parameter.

    Args:
        extraction_dir: Path to Q3-Omni extraction
        alpha: Exploration weight (0-1)
        num_generations: Number of generations to run

    Returns:
        dict with results
    """
    print(f"\n{'='*70}")
    print(f"EXTENDED VALIDATION: Œ±={alpha}")
    print(f"{'='*70}\n")

    # Create context classifier
    print("Creating context classifier...")
    classifier = ContextClassifier(num_contexts=5, embedding_dim=2048)

    # Fit with synthetic training data
    # CRITICAL: Convert to numpy float32 to prevent sklearn's float64 conversion
    training_embeddings = torch.randn(100, 2048).numpy().astype(np.float32)
    training_labels = torch.randint(0, 5, (100,)).numpy()
    classifier.fit(training_embeddings, training_labels)

    # Create trust selector
    print(f"Creating trust-based expert selector (Œ±={alpha})...")
    trust_selector = TrustBasedExpertSelector(
        num_experts=128,
        cache_size=16,
        component="thinker",
        context_classifier=classifier,
        exploration_weight=alpha
    )

    # Create model
    print("Initializing model with trust-based selection...")
    model = SelectiveLanguageModel(
        extraction_dir=extraction_dir,
        num_layers=1,
        max_loaded_experts=16,
        device="cpu",
        trust_selector=trust_selector
    )

    # Create quality measurer
    measurer = QualityMeasurement()

    # Test prompts
    prompts = create_test_prompts(num_generations)

    # Track results
    qualities = []
    trust_evolution = defaultdict(list)  # {expert_id: [trust_scores]}
    context_distribution = defaultdict(int)  # {context: count}

    print(f"\nRunning {num_generations} generations...\n")

    for gen_idx, prompt in enumerate(prompts):
        print(f"Generation {gen_idx+1}/{num_generations}: '{prompt[:30]}...'", end="", flush=True)

        try:
            # Tokenize (simplified)
            input_ids = torch.randint(0, 152064, (1, 10))
            output_ids = torch.randint(0, 152064, (1, 10))

            # Generate
            logits = model(input_ids, debug=False)

            # Measure quality
            quality = measurer.measure_perplexity(logits, output_ids)
            qualities.append(quality)

            # Track trust evolution for first 5 experts
            from sage.core.expert_reputation import get_default_reputation_db
            db = get_default_reputation_db()
            for expert_id in range(min(5, 128)):
                rep = db.get_reputation(expert_id, "thinker")
                if rep:
                    # Get context from last generation
                    # For now, use general context
                    trust = rep.get_context_trust("general", default=0.5)
                    trust_evolution[expert_id].append(trust)

            # Track context (get from classifier if available)
            # For now, infer from prompt type
            if any(kw in prompt.lower() for kw in ["def", "class", "import", "async"]):
                context = "code"
            elif any(kw in prompt.lower() for kw in ["insight", "understand", "theorem", "theory"]):
                context = "reasoning"
            else:
                context = "text"
            context_distribution[context] += 1

            print(f" ‚Üí Perplexity: {quality:.2f}, Context: {context}")

        except Exception as e:
            print(f" ‚ö†Ô∏è  Error: {e}")
            continue

    # Calculate statistics
    if not qualities:
        print("\n‚ùå No successful generations")
        return None

    avg_quality = np.mean(qualities)
    early_quality = np.mean(qualities[:10]) if len(qualities) >= 10 else avg_quality
    late_quality = np.mean(qualities[-10:]) if len(qualities) >= 10 else avg_quality
    improvement = ((early_quality - late_quality) / early_quality * 100) if early_quality > 0 else 0

    results = {
        'alpha': alpha,
        'num_generations': len(qualities),
        'avg_perplexity': float(avg_quality),
        'early_perplexity': float(early_quality),
        'late_perplexity': float(late_quality),
        'improvement_percent': float(improvement),
        'qualities': [float(q) for q in qualities],
        'trust_evolution': {int(k): [float(v) for v in vals] for k, vals in trust_evolution.items()},
        'context_distribution': dict(context_distribution)
    }

    print(f"\nResults (Œ±={alpha}):")
    print(f"  Average perplexity: {avg_quality:.2f}")
    print(f"  Early (1-10): {early_quality:.2f}")
    print(f"  Late (last 10): {late_quality:.2f}")
    print(f"  Improvement: {improvement:+.1f}%")
    print(f"  Contexts: {dict(context_distribution)}")

    return results


def run_parameter_sweep(extraction_dir, alphas=[0.1, 0.2, 0.3, 0.5, 0.7], num_generations=30):
    """
    Run parameter sweep over different alpha values.

    Args:
        extraction_dir: Path to Q3-Omni extraction
        alphas: List of alpha values to test
        num_generations: Generations per alpha

    Returns:
        List of results dicts
    """
    print("\n" + "="*70)
    print("PARAMETER SWEEP: Testing multiple Œ± values")
    print("="*70)
    print(f"\nAlpha values: {alphas}")
    print(f"Generations per alpha: {num_generations}")
    print(f"Total generations: {len(alphas) * num_generations}\n")

    all_results = []

    for alpha in alphas:
        result = run_extended_validation(extraction_dir, alpha, num_generations)
        if result:
            all_results.append(result)

        # Brief pause between runs
        time.sleep(2)

    return all_results


def analyze_results(all_results):
    """Analyze and compare results across different alpha values."""
    print("\n" + "="*70)
    print("ANALYSIS: Parameter Sweep Results")
    print("="*70 + "\n")

    if not all_results:
        print("‚ùå No results to analyze")
        return

    # Find best alpha
    best_result = min(all_results, key=lambda r: r['late_perplexity'])
    best_alpha = best_result['alpha']

    print("üìä Summary by Alpha:\n")
    print(f"{'Alpha':<8} {'Avg PPL':<10} {'Early':<10} {'Late':<10} {'Improve':<10}")
    print("-" * 50)

    for result in all_results:
        alpha = result['alpha']
        avg = result['avg_perplexity']
        early = result['early_perplexity']
        late = result['late_perplexity']
        improve = result['improvement_percent']

        marker = " ‚≠ê" if alpha == best_alpha else ""
        print(f"{alpha:<8.1f} {avg:<10.2f} {early:<10.2f} {late:<10.2f} {improve:<+10.1f}%{marker}")

    print(f"\n‚ú® Best Œ±: {best_alpha} (lowest late perplexity: {best_result['late_perplexity']:.2f})")
    print(f"   Improvement: {best_result['improvement_percent']:+.1f}%")

    # Learning effect analysis
    print("\nüìà Learning Effect:\n")
    for result in all_results:
        alpha = result['alpha']
        improve = result['improvement_percent']
        if improve > 10:
            print(f"  Œ±={alpha}: Strong learning ({improve:+.1f}%) ‚úÖ")
        elif improve > 0:
            print(f"  Œ±={alpha}: Moderate learning ({improve:+.1f}%)")
        else:
            print(f"  Œ±={alpha}: No improvement ({improve:+.1f}%) ‚ö†Ô∏è")

    # Context distribution
    print("\nüéØ Context Distribution:\n")
    for result in all_results[:1]:  # Just show first (should be similar)
        ctx_dist = result['context_distribution']
        total = sum(ctx_dist.values())
        for ctx, count in ctx_dist.items():
            pct = count / total * 100
            print(f"  {ctx:>10}: {count:>3} ({pct:>5.1f}%)")


def save_results(all_results, output_path="session63_results.json"):
    """Save results to JSON file."""
    output_file = Path(__file__).parent / output_path

    with open(output_file, 'w') as f:
        json.dump(all_results, f, indent=2)

    print(f"\nüíæ Results saved to: {output_file}")


def main():
    """Main execution."""
    print("="*70)
    print("Session 63: Extended Validation & Parameter Sweep")
    print("="*70)
    print("\nGoal: Validate robustness and optimize trust-based selection\n")

    # Setup
    extraction_dir = setup_extraction_dir()

    # Run parameter sweep
    # Start with smaller sweep for speed: [0.1, 0.3, 0.5]
    # Can expand to [0.1, 0.2, 0.3, 0.5, 0.7] if time permits
    alphas = [0.1, 0.3, 0.5]
    num_generations = 30

    all_results = run_parameter_sweep(extraction_dir, alphas, num_generations)

    # Analyze
    analyze_results(all_results)

    # Save
    save_results(all_results)

    print("\n" + "="*70)
    print("‚úÖ Session 63 complete!")
    print("="*70)


if __name__ == "__main__":
    main()
