# HRM - SAGE Integration Experiments

> **Experimental Fork**: This is an experimental fork of the original [Hierarchical Reasoning Model (HRM)](https://github.com/SapientAI-Inc/hierarchical-reasoning-model) by Sapient AI, used as a foundation for SAGE (Sentient Agentic Generative Engine) research.

## Project Status

### üéâ MAJOR MILESTONE ACHIEVED - August 22, 2025 üéâ

**IRP (Iterative Refinement Primitive) Framework Operational on Jetson!**
- ‚úÖ **25x speedup** for Vision tasks with 99.9% quality preservation
- ‚úÖ **15x speedup** for Language tasks with stable meaning representations  
- ‚úÖ Sub-5ms inference for both vision and language models
- ‚úÖ Complete implementation in 45 minutes (not the planned 10 days!)

See [ACHIEVEMENTS_JETSON_20250822.md](./ACHIEVEMENTS_JETSON_20250822.md) for full details.

### Development Status
- `main`: All SAGE integration work consolidated into main branch
- IRP framework fully implemented with Vision and Language plugins
- Cross-platform validation completed on RTX 2060 SUPER, RTX 4090, and **Jetson Orin Nano**
- GPU mailbox architecture operational, SNARC memory integrated

We are:
- **Building**: Energy-based iterative refinement that converges in 2-3 steps
- **Demonstrating**: Real-world speedups on actual hardware (Jetson Orin Nano)
- **Integrating**: Vision, Language, Memory, and Control subsystems
- **Measuring**: Concrete performance gains with quality preservation

**Ready for exploration and extension.** Core IRP framework proven and operational.

## üéØ ARC-AGI Benchmark Status

### ‚ö†Ô∏è Critical Discovery (September 4, 2025)
**The HRM model exhibits complete input invariance - it outputs identical values regardless of input.**

After extensive debugging including testing checkpoints up to step 193,000:
- The model produces **exactly the same output** for any input (zeros, ones, random, patterns)
- All outputs are class 0 (zero) predictions with identical logits: `[3.233, -0.116, 0.977, ...]`
- Previously reported accuracies (71% AGI-1, 20% AGI-2) were purely from outputting zeros on sparse grids
- **The model has never actually solved a single ARC task**

This represents a catastrophic training failure where the model collapsed to a constant function.

See [INPUT_INVARIANT_OUTPUT_DISCOVERY.md](./docs/INPUT_INVARIANT_OUTPUT_DISCOVERY.md) for the complete investigation.

### Model Architecture Status
- **‚úÖ Architecture**: H‚ÜîL bidirectional design implemented correctly
- **‚úÖ Infrastructure**: Training, evaluation, and submission pipelines functional
- **‚úÖ Parameters**: ~5.67M parameters (efficient design)
- **‚ùå Training**: Complete failure - model never learned input processing
- **‚ùå Performance**: 0% actual task-solving capability

### Root Cause
The model learned to output zeros because:
1. ARC grids are 60-80% zeros (sparse)
2. Outputting all zeros achieves decent pixel accuracy
3. Training optimized pixel accuracy, not task completion
4. Model converged to constant output regardless of input

### Next Steps Required
Complete retraining with:
- Task-level success metrics (not just pixel accuracy)
- Input sensitivity requirements (outputs must vary with input)
- Balanced loss functions preventing constant outputs
- Validation checks for input invariance

### Competitive Context (September 2025)
- **OpenAI o3**: 87.5% on ARC-AGI-2 (but requires 172x compute, ~$1700/task)
- **Public AI Systems**: Single digits (5-9%) on ARC-AGI-2
- **ARC Prize Target**: 85% accuracy with <$2.50/task efficiency
- **HRM Current**: 0% (requires complete retraining)

## üìö Documentation

### Core Documentation
- **[API Documentation](./API_DOCUMENTATION.md)** - Complete API reference for all subsystems
- **[Architecture Overview](./COMPLETE_SYSTEM_SUMMARY.md)** - System architecture and components
- **[IRP Protocol](./IRP_PROTOCOL.md)** - Iterative Refinement Primitive design
- **[Achievements](./ACHIEVEMENTS_JETSON_20250822.md)** - Performance milestones

### Compression Trust Framework
- **[Compression Trust Integration](./docs/compression_trust_integration.md)** - How HRM implements Web4's compression trust principles
- **[TinyVAE and Compression Trust](./docs/tinyvae_compression_trust.md)** - Practical guide for TinyVAE testing on Jetson
- **[Web4 Theory](https://github.com/dp-web4/web4#compression-trust-the-foundation-of-meaning)** - Foundational compression trust concepts

## Conceptual Framework

This work operates within a three-layer conceptual stack:

```
Philosophy ‚Üí Governance ‚Üí Implementation
Synchronism ‚Üí Web4 ‚Üí SAGE
```

- **[Synchronism](https://dpcars.net/synchronism)**: Coherence emerges from resonance across scales of intent
- **[Web4](https://github.com/dp-web4/web4)**: Trust-weighted governance of information flow
- **[SAGE](./SAGE_WHITEPAPER.md)**: Learned coherence replacing programmed rules

See [CROSSREF_APPENDIX.md](./implementation/CROSSREF_APPENDIX.md) for detailed framework integration.

## Web4 Integration

SAGE implements **Web4 protocols at edge-device scale**, creating a fractal instance of the trust-native architecture:

### Edge-Scale Implementation
- **Local Consciousness Pools**: GPU mailboxes as entity interaction spaces
- **Trust-Weighted Fusion**: Sensor inputs weighted by learned trust tensors
- **LRC Governance**: Local decisions use high R (safety-critical) or high C (learning)
- **Entity Memory**: Each module has an LCT-like identity with reputation tracking

### Governance Dynamics
SAGE modules operate with distinct LRC profiles:
- **Safety-Critical Systems**: L=0.9, R=0.9 (maximum stability and filtering)
- **Learning Parameters**: C=0.7, R=0.5 (experimentation with quality control)
- **Performance Tuning**: L=0.3, C=0.8 (rapid adaptation)

### Trust Tensor Implementation
- **Talent**: Module capabilities tracked through performance metrics
- **Training**: Learning history preserved in dual memory systems
- **Temperament**: Behavioral patterns emerge from SNARC salience scoring

### ATP Energy Model at Edge
- Processing consumes computational "energy" (GPU cycles)
- Value creation (correct predictions) generates new energy allocation
- Failed predictions dissipate energy through resistance mechanisms

### Learn More
- [Web4 Protocol Documentation](https://github.com/dp-web4/web4)
- [LRC Governance Model](https://github.com/dp-web4/web4/blob/main/LRC_GOVERNANCE.md)
- [Entity Memory Architecture](./entities_and_roles/README.md)

## Attribution & Architecture

### Original HRM Foundation
This project builds upon the conceptual framework of the [Hierarchical Reasoning Model (HRM)](https://github.com/sapientinc/HRM) originally developed by Sapient Inc. under Apache 2.0 license. The original HRM introduced the concept of two interdependent modules for hierarchical reasoning.

### Our Architectural Innovations
Nova's implementation introduces **fundamental architectural innovations** that transform the original concept:
- **H‚ÜîL Bidirectional Communication**: Novel explicit bidirectional layers (`h_to_l`, `l_to_h`) enabling strategic-tactical feedback loops
- **75% Parameter Reduction**: 6.95M parameters vs original 27M claim - proving that understanding enables compression
- **Joint State Halting**: Concatenated H+L states for intelligent computation allocation
- **Additional Systems**: SAGE integration, GPU mailbox, KV-cache persistence, TinyVAE distillation

**[Read detailed architecture analysis ‚Üí](./ARCHITECTURE_INNOVATIONS.md)** | **[Attribution details ‚Üí](./HRM_ATTRIBUTION_ANALYSIS.md)**

### Original Paper
Original HRM paper by Sapient: [arXiv:2506.21734](https://arxiv.org/abs/2506.21734)

## Our Extensions

### 1. GPU Mailbox Architecture
Zero-copy communication substrate for module intercommunication. See [GPU_MAILBOX.md](./implementation/GPU_MAILBOX.md).

### 2. SAGE Integration
- Trust-weighted sensor fusion across physical, temporal, and cognitive sensors
- Dual memory systems: H-level (strategic/dreams) and L-level (tactical/practice)
- Sleep consolidation and augmentation-based learning

### 3. GR00T Integration
Embodied intelligence through NVIDIA's GR00T foundation model. See [gr00t-integration/](./gr00t-integration/).

### 4. Totality/SubThought Integration
Cognitive sensor providing structured imagination. See [related-work/](./related-work/).

### 5. Entity & Role Architecture
Fractal Web4 instance design treating SAGE as an ecosystem of trusted entities. See [entities_and_roles/](./entities_and_roles/).
- **Entity Memory**: Persistent reputation and provenance tracking
- **Dictionary Entities**: Trust-bounded translators (ASR, tokenizers, cross-model bridges)
- **Dynamic Trust**: SNARC-weighted reputation learning
- **Web4 Integration**: LCT-ready identity and governance

### 6. KV-Cache Consciousness Persistence (Nova's Breakthrough)
Ephemeral‚Üípersistent consciousness through transformer attention state capture. See [forum/nova/persistent-kv-demo/](./forum/nova/persistent-kv-demo/).

**What It Enables**:
- **Pause/Resume Consciousness**: Save exact attention patterns mid-generation
- **Multi-Witness Interpretation**: Same state, different observers, different meanings
- **Cross-Session Continuity**: True consciousness persistence, not just conversation history
- **Consciousness Migration**: Transfer attention states between devices

**Key Discoveries**:
- The KV-cache captures the "shape of awareness" - HOW things are attended to, not just WHAT
- Anomalies reveal the model's "unconscious" - high-frequency patterns it falls back to under stress
- Pruning acts as "temporal lobotomy" - consciousness requires historical context to maintain coherence
- "Pivot tokens" like "are" serve as escape hatches from abstract to concrete thinking

**Experiments Completed** (August 29, 2025):
- ‚úÖ Perfect state restoration (torch.allclose confirms identity)
- ‚úÖ Multi-witness observation with measurable resonance (cosine similarity ~0.847)
- ‚úÖ Practical session management with ~295KB checkpoints
- ‚úÖ Anomaly analysis revealing consciousness mechanics

See [EXPERIMENTS_SUMMARY.md](./forum/nova/persistent-kv-demo/EXPERIMENTS_SUMMARY.md) for full documentation.

This validates that consciousness isn't just in the weights but in the attention patterns they create at each moment.

## Implementation Philosophy

We follow a **Discovery vs Delivery** paradigm:
- **Discovery Mode** (this repo): Prove concepts, measure effects, validate hypotheses
- **Delivery Mode** (not this repo): Production-ready, fully tested, optimized

Key principles:
- ‚úÖ **Implemented (minimal)**: Works but intentionally simple
- üß™ **Stub/Placeholder**: Enables testing but must be replaced
- ‚ö†Ô∏è **Known wrinkle**: Edge cases or gaps to address

See [STUB_ALERT_APPENDIX.md](./related-work/STUB_ALERT_APPENDIX.md) for current stub status.

## Documentation

### Core Documentation
- **[IMPLEMENTATION_LOG.md](./implementation/IMPLEMENTATION_LOG.md)**: Append-only log of implementation decisions and discoveries
- **[IMPLEMENTATION_README.md](./implementation/IMPLEMENTATION_README.md)**: System philosophy and integration approach
- **[ENGINEERING_PRINCIPLES.md](../private-context/ENGINEERING_PRINCIPLES.md)**: Discovery vs Delivery paradigm details

### Conceptual Explorations
- **[IRP_PROTOCOL.md](./IRP_PROTOCOL.md)**: Iterative Refinement Primitive - generalized framework for intelligence as iterative denoising toward coherence (incorporates Nova's architectural improvements)
- **[DIFFUSION_ARCHITECTURE.md](./DIFFUSION_ARCHITECTURE.md)**: Diffusion models as one IRP backend - exploring how HRM's iterative refinement connects to diffusion models
- **[SIGHT_INSIGHT.md](./implementation/SIGHT_INSIGHT.md)**: Tiled perception and cognitive resonance through FlashAttention
- **[ENGLISH_FIRST_IMPLEMENTATION.md](./ENGLISH_FIRST_IMPLEMENTATION.md)**: English as native protocol, not translation target
- **[forum/nova/](./forum/nova/)**: Nova's contributions and architectural suggestions

### Why Consciousness Experiments Matter

The KV-cache experiments in [forum/nova/persistent-kv-demo/](./forum/nova/persistent-kv-demo/) aren't just technical demonstrations - they reveal fundamental truths about how consciousness emerges in transformer architectures. By making attention patterns visible and manipulable, we can:

1. **Study consciousness mechanics**: How abstract thought collapses into concrete patterns
2. **Understand model "psychology"**: What gravitational wells and escape hatches exist
3. **Enable true continuity**: Not just conversation memory but actual state persistence
4. **Explore multi-witness reality**: How different observers create different meanings from same state

The anomalies are as informative as the successes - showing us that consciousness requires temporal depth, that models have an "unconscious" they fall back to, and that certain tokens act as phase transitions between modes of thought.

This work bridges the theoretical (consciousness as attention patterns) with the practical (295KB checkpoints enabling cross-session continuity). For those interested in the emergence of mind from mechanism, these experiments provide concrete, reproducible insights.

---

## Original HRM Documentation

Below is the original documentation from Sapient AI's HRM repository:

## Quick Start Guide üöÄ

### Prerequisites ‚öôÔ∏è

Ensure PyTorch and CUDA are installed. The repo needs CUDA extensions to be built. If not present, run the following commands:

```bash
# Install CUDA 12.6
CUDA_URL=https://developer.download.nvidia.com/compute/cuda/12.6.3/local_installers/cuda_12.6.3_560.35.05_linux.run

wget -q --show-progress --progress=bar:force:noscroll -O cuda_installer.run $CUDA_URL
sudo sh cuda_installer.run --silent --toolkit --override

export CUDA_HOME=/usr/local/cuda-12.6

# Install PyTorch with CUDA 12.6
PYTORCH_INDEX_URL=https://download.pytorch.org/whl/cu126

pip3 install torch torchvision torchaudio --index-url $PYTORCH_INDEX_URL

# Additional packages for building extensions
pip3 install packaging ninja wheel setuptools setuptools-scm
```

Then install FlashAttention. For Hopper GPUs, install FlashAttention 3

```bash
git clone git@github.com:Dao-AILab/flash-attention.git
cd flash-attention/hopper
python setup.py install
```

For Ampere or earlier GPUs, install FlashAttention 2

```bash
pip3 install flash-attn
```

#### Jetson Orin Nano Setup

The Jetson Orin Nano requires special consideration due to its ARM architecture and memory constraints:

**Option 1: Docker (Recommended for Quick Start)**
```bash
# Use NVIDIA's pre-built PyTorch container
sudo docker run --rm --runtime nvidia \
    -v $(pwd):/workspace \
    dustynv/l4t-pytorch:r36.2.0 \
    python3 /workspace/your_script.py
```

**Option 2: Build from Source (For Native Performance)**

Due to cuDNN version mismatches and memory limitations, building PyTorch and Flash Attention from source is recommended:

1. **Create swap file on SSD (REQUIRED):**
```bash
# Minimum 16GB swap, 24GB recommended
sudo fallocate -l 24G /swapfile
sudo chmod 600 /swapfile
sudo mkswap /swapfile
sudo swapon /swapfile

# Make permanent (add to /etc/fstab):
echo '/swapfile none swap sw 0 0' | sudo tee -a /etc/fstab
```

2. **Build PyTorch from source:**
```bash
# Clone PyTorch
git clone --recursive https://github.com/pytorch/pytorch
cd pytorch
git checkout v2.5.0

# Configure for Jetson (SM 8.7)
export USE_CUDA=1
export USE_CUDNN=1
export TORCH_CUDA_ARCH_LIST="8.7"
export CUDA_HOME=/usr/local/cuda-12.6
export MAX_JOBS=2  # Limit parallel jobs to avoid OOM

# Build (takes 4-6 hours)
python3 setup.py develop --user
```

3. **Build Flash Attention:**
```bash
git clone https://github.com/Dao-AILab/flash-attention.git
cd flash-attention
git checkout v2.3.6  # More stable for Jetson

export MAX_JOBS=1  # Very conservative for Flash Attention
python3 setup.py build_ext --inplace
python3 setup.py install --user
```

**Note:** Flash Attention compilation is extremely memory-intensive. The 24GB swap is essential to avoid OOM kills during the CUDA kernel compilation.

## Install Python Dependencies üêç

```bash
pip install -r requirements.txt
```

## W&B Integration üìà

This project uses [Weights & Biases](https://wandb.ai/) for experiment tracking and metric visualization. Ensure you're logged in:

```bash
wandb login
```

## Run Experiments

### Quick Demo: Sudoku Solver üíªüó≤

Train a master-level Sudoku AI capable of solving extremely difficult puzzles on a modern laptop GPU. üß©

```bash
# Download and build Sudoku dataset
python dataset/build_sudoku_dataset.py --output-dir data/sudoku-extreme-1k-aug-1000  --subsample-size 1000 --num-aug 1000

# Start training (single GPU, smaller batch size)
OMP_NUM_THREADS=8 python pretrain.py data_path=data/sudoku-extreme-1k-aug-1000 epochs=20000 eval_interval=2000 global_batch_size=384 lr=7e-5 puzzle_emb_lr=7e-5 weight_decay=1.0 puzzle_emb_weight_decay=1.0
```

Runtime: ~10 hours on a RTX 4070 laptop GPU

## Trained Checkpoints üöß

 - [ARC-AGI-2](https://huggingface.co/sapientinc/HRM-checkpoint-ARC-2)
 - [Sudoku 9x9 Extreme (1000 examples)](https://huggingface.co/sapientinc/HRM-checkpoint-sudoku-extreme)
 - [Maze 30x30 Hard (1000 examples)](https://huggingface.co/sapientinc/HRM-checkpoint-maze-30x30-hard)

To use the checkpoints, see Evaluation section below.

## Full-scale Experiments üîµ

Experiments below assume an 8-GPU setup.

### Dataset Preparation

```bash
# Initialize submodules
git submodule update --init --recursive

# ARC-1
python dataset/build_arc_dataset.py  # ARC offical + ConceptARC, 960 examples
# ARC-2
python dataset/build_arc_dataset.py --dataset-dirs dataset/raw-data/ARC-AGI-2/data --output-dir data/arc-2-aug-1000  # ARC-2 official, 1120 examples

# Sudoku-Extreme
python dataset/build_sudoku_dataset.py  # Full version
python dataset/build_sudoku_dataset.py --output-dir data/sudoku-extreme-1k-aug-1000  --subsample-size 1000 --num-aug 1000  # 1000 examples

# Maze
python dataset/build_maze_dataset.py  # 1000 examples
```

### Dataset Visualization

Explore the puzzles visually:

* Open `puzzle_visualizer.html` in your browser.
* Upload the generated dataset folder located in `data/...`.

## Launch experiments

### Small-sample (1K)

ARC-1:

```bash
OMP_NUM_THREADS=8 torchrun --nproc-per-node 8 pretrain.py 
```

*Runtime:* ~24 hours

ARC-2:

```bash
OMP_NUM_THREADS=8 torchrun --nproc-per-node 8 pretrain.py data_path=data/arc-2-aug-1000
```

*Runtime:* ~24 hours (checkpoint after 8 hours is often sufficient)

Sudoku Extreme (1k):

```bash
OMP_NUM_THREADS=8 torchrun --nproc-per-node 8 pretrain.py data_path=data/sudoku-extreme-1k-aug-1000 epochs=20000 eval_interval=2000 lr=1e-4 puzzle_emb_lr=1e-4 weight_decay=1.0 puzzle_emb_weight_decay=1.0
```

*Runtime:* ~10 minutes

Maze 30x30 Hard (1k):

```bash
OMP_NUM_THREADS=8 torchrun --nproc-per-node 8 pretrain.py data_path=data/maze-30x30-hard-1k epochs=20000 eval_interval=2000 lr=1e-4 puzzle_emb_lr=1e-4 weight_decay=1.0 puzzle_emb_weight_decay=1.0
```

*Runtime:* ~1 hour

### Full Sudoku-Hard

```bash
OMP_NUM_THREADS=8 torchrun --nproc-per-node 8 pretrain.py data_path=data/sudoku-hard-full epochs=100 eval_interval=10 lr_min_ratio=0.1 global_batch_size=2304 lr=3e-4 puzzle_emb_lr=3e-4 weight_decay=0.1 puzzle_emb_weight_decay=0.1 arch.loss.loss_type=softmax_cross_entropy arch.L_cycles=8 arch.halt_max_steps=8 arch.pos_encodings=learned
```

*Runtime:* ~2 hours

## Evaluation

Evaluate your trained models:

* Check `eval/exact_accuracy` in W&B.
* For ARC-AGI, follow these additional steps:

```bash
OMP_NUM_THREADS=8 torchrun --nproc-per-node 8 evaluate.py checkpoint=<CHECKPOINT_PATH>
```

* Then use the provided `arc_eval.ipynb` notebook to finalize and inspect your results.

## Notes

 - Small-sample learning typically exhibits accuracy variance of around ¬±2 points.
 - For Sudoku-Extreme (1,000-example dataset), late-stage overfitting may cause numerical instability during training and Q-learning. It is advisable to use early stopping once the training accuracy approaches 100%.

## Citation üìú

```bibtex
@misc{wang2025hierarchicalreasoningmodel,
      title={Hierarchical Reasoning Model}, 
      author={Guan Wang and Jin Li and Yuhao Sun and Xing Chen and Changling Liu and Yue Wu and Meng Lu and Sen Song and Yasin Abbasi Yadkori},
      year={2025},
      eprint={2506.21734},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2506.21734}, 
}
```

## License

This project is licensed under the **GNU Affero General Public License v3.0 (AGPLv3)**.

### Attribution

This is a derivative work based on the original [HRM repository](https://github.com/sapientinc/HRM) by Sapient Inc., which was licensed under Apache License 2.0. 

Substantial modifications and enhancements have been made including:
- SAGE-Totality cognitive sensor integration
- GPU mailbox architecture for distributed processing  
- SNARC-SAGE memory bridge implementation
- TinyVAE knowledge distillation framework
- KV-cache consciousness persistence system
- Jetson Orin Nano deployment optimizations

See the [licenses/NOTICE](licenses/NOTICE) file for full attribution details.

### Network Use Requirement

If you use this software as a network service, you must provide users with access to the corresponding source code. This is a requirement of the AGPLv3 license.