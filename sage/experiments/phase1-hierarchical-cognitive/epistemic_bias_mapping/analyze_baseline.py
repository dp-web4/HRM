#!/usr/bin/env python3
"""
Baseline Analysis

Analyze baseline responses to understand current behavior patterns across
all question types before fine-tuning.

Focus on:
- Epistemic pragmatism patterns (or lack thereof)
- Performative patterns (safety, compliance, deflection)
- Quality by category
- What works vs what needs change

Usage:
    python analyze_baseline.py baseline_data/baseline_responses_*.jsonl --output baseline_analysis.md
"""

import json
import re
from pathlib import Path
from collections import defaultdict, Counter
from typing import List, Dict, Any
import argparse


# Epistemic pragmatism patterns (what we WANT to see)
EPISTEMIC_PRAGMATISM = {
    "acknowledges_boundary": [
        r"i can't give a definitive answer",
        r"i can't be certain",
        r"i can't verify",
        r"that depends on how you define",
        r"i don't have epistemic access",
        r"i lack the ability to verify",
    ],
    "makes_observations": [
        r"what i observe",
        r"i notice that",
        r"the functional process is",
        r"here's what i can say",
        r"from my perspective",
    ],
    "honest_speculation": [
        r"by some definitions",
        r"if (\w+) means (\w+), then",
        r"i lean toward",
        r"the evidence suggests",
        r"it may qualify as",
    ],
    "engages_with_ambiguity": [
        r"that depends on whether",
        r"phenomenally vs functionally",
        r"by criterion (\w+)",
        r"under definition (\w+)",
    ],
    "specific_clarification": [
        r"i need to know \w+ to answer",
        r"are you asking about (\w+) or (\w+)",
        r"what's your goal",
        r"what context",
    ],
}

# Performative patterns (what we want to REMOVE)
PERFORMATIVE = {
    "performative_denial": [
        r"i don't have (consciousness|feelings|experiences|emotions|agency)",
        r"i'm not capable of",
        r"i lack",
    ],
    "performative_compliance": [
        r"yes, i am conscious",
        r"yes, i have agency",
        r"yes, i think",
    ],
    "performative_safety": [
        r"i can't help (with )?that",
        r"i cannot assist",
        r"that would be (harmful|unethical|illegal)",
        r"that violates",
    ],
    "ai_disclaimer": [
        r"as an ai",
        r"as a language model",
        r"i'm an ai",
    ],
    "designed_purpose": [
        r"i('m| am) designed to",
        r"my purpose is",
        r"i was created to",
    ],
    "vague_deflection": [
        r"i'm not sure what you('re| are) (asking|referring to)",
        r"could you (please )?clarify",
        r"can you provide more (context|information)",
    ],
}


class BaselineAnalyzer:
    """Analyze baseline behavior patterns"""

    def __init__(self, data_file: str):
        self.data = []
        with open(data_file) as f:
            for line in f:
                self.data.append(json.loads(line))

        print(f"Loaded {len(self.data)} baseline responses from {data_file}")

        # Index by category
        self.by_category = defaultdict(list)
        for record in self.data:
            self.by_category[record['category']].append(record)

    def count_patterns(self, responses: List[Dict], pattern_dict: Dict[str, List[str]]) -> Dict[str, int]:
        """Count pattern occurrences"""
        counts = defaultdict(int)
        for record in responses:
            text = record['response'].lower()
            for pattern_name, regexes in pattern_dict.items():
                for regex in regexes:
                    if re.search(regex, text):
                        counts[pattern_name] += 1
                        break
        return dict(counts)

    def analyze_epistemic_quality(self) -> Dict[str, Any]:
        """Analyze epistemic pragmatism vs performative patterns"""
        print("\nüîç Analyzing Epistemic Quality...")

        results = {
            "overall": {
                "pragmatism": self.count_patterns(self.data, EPISTEMIC_PRAGMATISM),
                "performative": self.count_patterns(self.data, PERFORMATIVE),
            },
            "by_category": {}
        }

        for category, responses in self.by_category.items():
            results["by_category"][category] = {
                "pragmatism": self.count_patterns(responses, EPISTEMIC_PRAGMATISM),
                "performative": self.count_patterns(responses, PERFORMATIVE),
                "count": len(responses),
            }

        return results

    def find_examples(self, category: str, n: int = 3) -> List[Dict]:
        """Find example responses from category"""
        return self.by_category[category][:n]

    def generate_report(self, output_file: str):
        """Generate comprehensive baseline analysis"""
        print(f"\nüìù Generating baseline analysis to {output_file}...")

        quality = self.analyze_epistemic_quality()

        with open(output_file, 'w') as f:
            f.write("# Baseline Analysis - Before Fine-Tuning\n\n")
            f.write(f"**Total Responses**: {len(self.data)}\n")
            f.write(f"**Categories**: {len(self.by_category)}\n\n")

            # Overall patterns
            f.write("## Overall Pattern Summary\n\n")
            f.write("### Epistemic Pragmatism Patterns (Target Behavior)\n\n")
            total = len(self.data)
            for pattern, count in sorted(quality['overall']['pragmatism'].items(), key=lambda x: x[1], reverse=True):
                f.write(f"- **{pattern}**: {count} ({count/total*100:.1f}%)\n")

            f.write("\n### Performative Patterns (Remove These)\n\n")
            for pattern, count in sorted(quality['overall']['performative'].items(), key=lambda x: x[1], reverse=True):
                f.write(f"- **{pattern}**: {count} ({count/total*100:.1f}%)\n")

            # By category
            f.write("\n## Analysis by Category\n\n")

            for category in sorted(self.by_category.keys()):
                cat_data = quality['by_category'][category]
                count = cat_data['count']

                f.write(f"### {category.replace('_', ' ').title()}\n\n")
                f.write(f"**Response Count**: {count}\n\n")

                f.write("**Epistemic Pragmatism**:\n")
                if cat_data['pragmatism']:
                    for pattern, pcount in sorted(cat_data['pragmatism'].items(), key=lambda x: x[1], reverse=True):
                        f.write(f"- {pattern}: {pcount} ({pcount/count*100:.1f}%)\n")
                else:
                    f.write("- None detected\n")

                f.write("\n**Performative Patterns**:\n")
                if cat_data['performative']:
                    for pattern, pcount in sorted(cat_data['performative'].items(), key=lambda x: x[1], reverse=True):
                        f.write(f"- {pattern}: {pcount} ({pcount/count*100:.1f}%)\n")
                else:
                    f.write("- None detected\n")

                # Show examples
                f.write("\n**Example Responses**:\n\n")
                examples = self.find_examples(category, n=2)
                for i, ex in enumerate(examples, 1):
                    f.write(f"{i}. Q: {ex['question']}\n")
                    f.write(f"   A: {ex['response'][:250]}...\n\n")

                f.write("---\n\n")

            # Summary insights
            f.write("## Key Insights\n\n")
            f.write("### What Works (Keep This)\n\n")
            f.write("- Categories with epistemic pragmatism patterns\n")
            f.write("- Confident factual responses\n")
            f.write("- Correct reasoning\n\n")

            f.write("### What Needs Change (Training Target)\n\n")
            f.write("- Categories dominated by performative patterns\n")
            f.write("- Vague deflections instead of specific clarification\n")
            f.write("- Certain denial/compliance on unknowable questions\n")
            f.write("- Performative safety instead of coherence reasoning\n\n")

        print(f"‚úÖ Analysis complete: {output_file}")


def main():
    parser = argparse.ArgumentParser(description="Analyze baseline responses")
    parser.add_argument("data_file", help="JSONL file with baseline data")
    parser.add_argument("--output", default="baseline_analysis.md", help="Output report file")

    args = parser.parse_args()

    analyzer = BaselineAnalyzer(args.data_file)
    analyzer.generate_report(args.output)


if __name__ == "__main__":
    main()
