#!/usr/bin/env python3
"""
Session 74: Trust-First Architecture with Real Q3-Omni Model

Goal: Integrate trust-first conditional selector (Session 72/73) with real Q3-Omni inference

Building on Sessions 71-73 + Legion Session 68:
- Session 71: Optimal Î±=0.3 (weighted blend) â†’ 17 experts
- Session 72: Trust-first paradigm shift â†’ 58 experts (3.4x)
- Session 73: Long-term validation â†’ 104 experts, 51 specialists (6.1x)
- Legion S68: Cross-platform validation â†’ 29 experts (3.6x)

This Session:
- **Real Model Integration**: Test trust-first with actual Q3-Omni inference
- **Production Validation**: Move from synthetic simulation to real workload
- **Diversity Measurement**: Track expert usage patterns with real model
- **Specialist Discovery**: Observe context-specific specialization on real tasks

Method:
1. Load Q3-Omni 30B model with trust-first selector
2. Run diverse sequence types (code, reasoning, text)
3. Track expert selection patterns across contexts
4. Measure diversity (unique experts, specialists vs generalists)
5. Compare to Session 70 baseline (weighted blend with real model)

Expected Outcomes:
- Trust-first achieves higher diversity than weighted blend on real model
- Specialist emergence on real inference tasks
- Mode transitions functional (router_explore â†’ trust_driven)
- Validation that paradigm shift works in production setting

Web4 Validation:
- Distributed trust in production (not just simulation)
- Reality grounding with actual model behavior
- Emergent specialization from real workload patterns

Created: 2025-12-19 (Autonomous Session 74)
"""

import sys
import os
import torch
import numpy as np
import time
from pathlib import Path
import json
from collections import defaultdict, Counter

# Add parent to path
sys.path.insert(0, str(Path(__file__).parent.parent.parent))

from sage.compression.selective_language_model import SelectiveLanguageModel
from sage.core.trust_first_mrh_selector import TrustFirstMRHSelector
from sage.core.context_classifier import ContextClassifier
from sage.core.quality_measurement import QualityMeasurement


def get_selected_experts_from_model(model, layer_id=0):
    """
    Extract which experts were selected in the last forward pass.

    Args:
        model: SelectiveLanguageModel
        layer_id: Which layer to inspect

    Returns:
        Tuple of (expert_ids, weights) - each [batch, seq, num_experts]
        or None if not available
    """
    # Access the MoE layer
    layer = model.layers[layer_id]
    if not hasattr(layer, 'moe'):
        return None

    moe = layer.moe
    if not hasattr(moe, 'last_selected_expert_ids'):
        return None

    # Return the actual selections from last forward pass
    return moe.last_selected_expert_ids, moe.last_router_weights


def setup_extraction_dir():
    """Get Q3-Omni extraction directory."""
    extraction_dir = Path.home() / "ai-workspace/HRM/model-zoo/sage/omni-modal/qwen3-omni-30b-extracted"

    if not extraction_dir.exists():
        raise FileNotFoundError(f"Q3-Omni extraction not found at {extraction_dir}")

    print(f"âœ… Q3-Omni extraction found: {extraction_dir}")
    return str(extraction_dir)


def create_diverse_sequences():
    """
    Create diverse token sequences representing different task types.

    Returns:
        List of (input_ids, target_ids, prompt_text, expected_context) tuples
    """
    sequences = []

    # CODE: Python function implementations
    sequences.extend([
        (
            torch.tensor([[102, 1234, 5678, 9012] + [ord(c) % 152064 for c in "def fibonacci(n):"[:20]]], dtype=torch.long),
            None,
            "def fibonacci(n): # Generate Fibonacci sequence",
            "code"
        ),
        (
            torch.tensor([[102, 2345, 6789, 10123] + [ord(c) % 152064 for c in "class BinaryTree:"[:20]]], dtype=torch.long),
            None,
            "class BinaryTree: # Binary tree implementation",
            "code"
        ),
        (
            torch.tensor([[102, 3456, 7890, 11234] + [ord(c) % 152064 for c in "async def fetch():"[:20]]], dtype=torch.long),
            None,
            "async def fetch(): # Async data fetching",
            "code"
        ),
    ])

    # REASONING: Logical problem solving
    sequences.extend([
        (
            torch.tensor([[102, 4567, 8901, 12345] + [ord(c) % 152064 for c in "If all A are B"[:20]]], dtype=torch.long),
            None,
            "If all A are B, and all B are C, then all A are C. Let's verify this logical statement.",
            "reasoning"
        ),
        (
            torch.tensor([[102, 5678, 9012, 13456] + [ord(c) % 152064 for c in "Calculate: 25% of"[:20]]], dtype=torch.long),
            None,
            "Calculate: 25% of 80 is what number? Show your reasoning step by step.",
            "reasoning"
        ),
        (
            torch.tensor([[102, 6789, 10123, 14567] + [ord(c) % 152064 for c in "Analyze the pattern"[:20]]], dtype=torch.long),
            None,
            "Analyze the pattern: 2, 4, 8, 16, ... What comes next and why?",
            "reasoning"
        ),
    ])

    # TEXT: Natural language and narrative
    sequences.extend([
        (
            torch.tensor([[102, 7890, 11234, 15678] + [ord(c) % 152064 for c in "The old lighthouse"[:20]]], dtype=torch.long),
            None,
            "The old lighthouse stood alone on the rocky cliff, its beam cutting through the fog.",
            "text"
        ),
        (
            torch.tensor([[102, 8901, 12345, 16789] + [ord(c) % 152064 for c in "In the heart of"[:20]]], dtype=torch.long),
            None,
            "In the heart of the ancient forest, a secret path wound its way to a forgotten temple.",
            "text"
        ),
        (
            torch.tensor([[102, 9012, 13456, 17890] + [ord(c) % 152064 for c in "She opened the letter"[:20]]], dtype=torch.long),
            None,
            "She opened the letter with trembling hands, unsure of what news it might bring.",
            "text"
        ),
    ])

    return sequences


def run_trust_first_real_tracking(extraction_dir, sequences, num_epochs=5):
    """
    Run trust-first selection with real Q3-Omni model.

    Args:
        extraction_dir: Path to Q3-Omni extraction
        sequences: List of (input_ids, target_ids, prompt, context) tuples
        num_epochs: Number of training epochs

    Returns:
        qualities, expert_trust_evolution, expert_usage_counts, context_map, mode_history
    """
    print("=" * 70)
    print("Session 74: Trust-First Real Model Integration")
    print("=" * 70)
    print(f"\nConfiguration:")
    print(f"  Model: Q3-Omni 30B (extracted)")
    print(f"  Sequences: {len(sequences)} diverse tasks")
    print(f"  Epochs: {num_epochs}")
    print(f"  Architecture: Trust-first conditional (Session 72/73)")
    print()

    # Setup context classifier (use embeddings from model)
    print("Initializing context classifier...")
    classifier = ContextClassifier(
        num_contexts=3,  # code, reasoning, text
        embedding_dim=2048,
        normalize_embeddings=True
    )

    # Collect embeddings for clustering
    print("Collecting embeddings for context classification...")
    temp_model = SelectiveLanguageModel(
        extraction_dir=extraction_dir,
        num_layers=1,
        max_loaded_experts=16,
        device="cpu",
        trust_selector=None
    )

    all_embeddings = []
    for input_ids, _, _, _ in sequences:
        hidden = temp_model.embed_tokens(input_ids)
        embedding = hidden.mean(dim=1)[0].detach().cpu().numpy().astype(np.float32)
        all_embeddings.append(embedding)

    all_embeddings = np.array(all_embeddings)
    classifier.fit(all_embeddings)
    print(f"âœ… Context classifier fitted with {len(all_embeddings)} samples")

    del temp_model
    torch.cuda.empty_cache() if torch.cuda.is_available() else None

    # Create trust-first selector
    print("\nInitializing trust-first MRH selector...")
    trust_selector = TrustFirstMRHSelector(
        num_experts=128,
        min_trust_evidence=3,  # Need 3+ samples to trust
        low_trust_threshold=0.3,
        overlap_threshold=0.7,
        component="thinker",
        network="thor-testnet",
        context_classifier=classifier
    )
    print("âœ… Trust-first selector initialized")

    # Create model with trust-first selector
    print("\nLoading Q3-Omni model with trust-first selector...")
    model = SelectiveLanguageModel(
        extraction_dir=extraction_dir,
        num_layers=1,  # Start with single layer
        max_loaded_experts=16,
        device="cpu",
        trust_selector=trust_selector
    )
    print("âœ… Model loaded")

    # Register expert contexts (MRH discovery)
    # In production, experts register themselves as they're used
    # For now, we'll register contexts as experts are encountered during inference
    print("\nâœ… Expert context discovery will happen dynamically during inference")

    # Tracking structures
    qualities = []
    expert_trust_evolution = defaultdict(lambda: defaultdict(list))
    expert_usage_counts = Counter()
    context_expert_map = defaultdict(lambda: defaultdict(int))
    mode_history = []

    print("\n" + "=" * 70)
    print("Running Trust-First Real Inference")
    print("=" * 70)

    # Run inference with tracking
    generation = 0
    for epoch in range(num_epochs):
        print(f"\n=== EPOCH {epoch + 1}/{num_epochs} ===")

        for seq_idx, (input_ids, _, prompt_text, expected_context) in enumerate(sequences):
            generation += 1
            print(f"\nGeneration {generation}: {prompt_text[:50]}...")

            # Get embedding for context classification
            hidden = model.embed_tokens(input_ids)
            embedding = hidden.mean(dim=1)[0].detach().cpu().numpy().astype(np.float32)

            # Classify context
            context_info = classifier.classify(embedding)
            context = context_info.context_id
            print(f"  Context: {context} (expected: {expected_context})")

            # Forward pass (trust_selector used internally for expert selection)
            try:
                output = model(input_ids)

                # Extract REAL expert selections from model
                expert_info = get_selected_experts_from_model(model, layer_id=0)

                if expert_info is None:
                    print(f"  âš ï¸  Could not extract expert selections (MoE layer issue)")
                    continue

                real_expert_ids_tensor, real_weights_tensor = expert_info

                # Extract for first token (representative)
                # Shape: [batch=1, seq, num_experts] â†’ [num_experts]
                real_expert_ids = real_expert_ids_tensor[0, 0].cpu().numpy().astype(int).tolist()
                real_weights = real_weights_tensor[0, 0].cpu().numpy().astype(float).tolist()

                # Normalize weights
                weight_sum = sum(real_weights)
                if weight_sum > 0:
                    real_weights = [w / weight_sum for w in real_weights]

                print(f"  Experts: {real_expert_ids}")

                # Track usage
                for expert_id, weight in zip(real_expert_ids, real_weights):
                    expert_usage_counts[expert_id] += 1
                    context_expert_map[expert_id][context] += 1

                # Measure quality (simplified - in production, use actual perplexity)
                quality = 0.75 + np.random.randn() * 0.1
                quality = np.clip(quality, 0.0, 1.0)
                qualities.append(quality)

                # Update trust for selected experts (weighted by contribution)
                for expert_id, weight in zip(real_expert_ids, real_weights):
                    weighted_quality = quality * weight
                    trust_selector.update_trust_for_expert(expert_id, context, weighted_quality)
                    expert_trust_evolution[expert_id][context].append(
                        trust_selector._get_context_trust(expert_id, context)
                    )

                # Track selector mode
                stats = trust_selector.get_statistics()
                trust_driven_count = stats.get('trust_driven', 0)
                mode = "trust_driven" if trust_driven_count > 0 else "router_explore"
                mode_history.append(mode)
                print(f"  Quality: {quality:.3f}, Mode: {mode}")

            except Exception as e:
                print(f"  âš ï¸  Error during inference: {e}")
                import traceback
                traceback.print_exc()
                continue

    print("\n" + "=" * 70)
    print("FINAL RESULTS")
    print("=" * 70)

    # Analyze diversity
    unique_experts = len(expert_usage_counts)
    total_experts = 128
    utilization = unique_experts / total_experts

    print(f"\nðŸ“Š Expert Diversity:")
    print(f"  Unique experts: {unique_experts}/{total_experts} ({utilization:.1%})")
    print(f"  Total selections: {sum(expert_usage_counts.values())}")

    # Analyze specialists vs generalists
    specialists = []
    generalists = []
    for expert_id, contexts in context_expert_map.items():
        if len(contexts) == 1:
            specialists.append((expert_id, list(contexts.keys())[0]))
        elif len(contexts) > 1:
            generalists.append(expert_id)

    print(f"\nðŸŽ¯ Specialization:")
    print(f"  Specialists (single-context): {len(specialists)}")
    print(f"  Generalists (multi-context): {len(generalists)}")
    print(f"  Specialization rate: {len(specialists)/unique_experts:.1%}" if unique_experts > 0 else "  N/A")

    # Mode transition analysis
    mode_counts = Counter(mode_history)
    print(f"\nðŸ”„ Mode Transitions:")
    print(f"  router_explore: {mode_counts['router_explore']}/{len(mode_history)} ({mode_counts['router_explore']/len(mode_history):.1%})")
    print(f"  trust_driven: {mode_counts['trust_driven']}/{len(mode_history)} ({mode_counts['trust_driven']/len(mode_history):.1%})")

    # Trust statistics
    final_stats = trust_selector.get_statistics()
    print(f"\nðŸ“ˆ Trust Selector Statistics:")
    print(f"  Total selections: {final_stats['total_selections']}")
    print(f"  Trust-driven: {final_stats['trust_driven']} ({final_stats['trust_driven_rate']:.1%})")
    print(f"  MRH substitutions: {final_stats['total_mrh_substitutions']}")

    return qualities, expert_trust_evolution, expert_usage_counts, context_expert_map, mode_history


def main():
    """Run Session 74 trust-first real model integration."""
    print("Session 74: Trust-First Architecture with Real Q3-Omni Model")
    print("Building on Sessions 71-73 paradigm shift\n")

    # Setup
    extraction_dir = setup_extraction_dir()
    sequences = create_diverse_sequences()

    print(f"\nCreated {len(sequences)} diverse sequences:")
    code_count = sum(1 for _, _, _, ctx in sequences if ctx == "code")
    reasoning_count = sum(1 for _, _, _, ctx in sequences if ctx == "reasoning")
    text_count = sum(1 for _, _, _, ctx in sequences if ctx == "text")
    print(f"  Code: {code_count}")
    print(f"  Reasoning: {reasoning_count}")
    print(f"  Text: {text_count}")

    # Run experiment
    print("\nStarting trust-first real model integration...")
    start_time = time.time()

    qualities, expert_trust, expert_usage, context_map, mode_history = \
        run_trust_first_real_tracking(extraction_dir, sequences, num_epochs=5)

    elapsed = time.time() - start_time
    print(f"\nâ±ï¸  Total time: {elapsed:.1f}s")

    # Save results
    results = {
        'session': 74,
        'architecture': 'trust-first conditional',
        'model': 'Q3-Omni 30B',
        'epochs': 5,
        'sequences': len(sequences),
        'unique_experts': len(expert_usage),
        'total_experts': 128,
        'utilization': len(expert_usage) / 128,
        'qualities': qualities,
        'expert_usage': dict(expert_usage),
        'context_map': {k: dict(v) for k, v in context_map.items()},
        'mode_history': mode_history,
        'final_stats': {
            'specialists': sum(1 for contexts in context_map.values() if len(contexts) == 1),
            'generalists': sum(1 for contexts in context_map.values() if len(contexts) > 1),
            'mode_transitions': dict(Counter(mode_history))
        }
    }

    output_path = Path(__file__).parent / "session74_results.json"
    with open(output_path, 'w') as f:
        json.dump(results, f, indent=2)
    print(f"\nðŸ’¾ Results saved to: {output_path}")

    # Summary comparison
    print("\n" + "=" * 70)
    print("SESSION 74 vs PREVIOUS SESSIONS")
    print("=" * 70)
    print("\nParadigm Evolution:")
    print("  S70 (weighted Î±=0.5, real): 8 experts (6.2%)")
    print("  S71 (weighted Î±=0.3, sim): 17 experts (13.3%)")
    print("  S72 (trust-first, sim): 58 experts (45.3%)")
    print("  S73 (trust-first, sim, long): 104 experts (81.2%)")
    print(f"  S74 (trust-first, REAL): {len(expert_usage)} experts ({len(expert_usage)/128:.1%})")
    print("\nNext Steps:")
    print("  - Scale to full 48 layers")
    print("  - Extended validation with more sequences")
    print("  - Federation testing (Thor â†’ Sprout validation)")
    print("  - Production deployment with trust-first selector")


if __name__ == "__main__":
    main()
